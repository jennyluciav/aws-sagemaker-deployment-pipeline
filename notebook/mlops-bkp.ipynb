{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MLOps Deployment Pipeline\r\n",
    "\r\n",
    "\r\n",
    "## Overview\r\n",
    "\r\n",
    "En este notebook, iremos paso a paso por un pipeline de MLOps para construir, entrenar, implementar y monitorear un modelo de regresión XGBoost que predice la tarifa de taxi esperada usando el [dataset](https://registry.opendata.aws/nyc-tlc-trip-records-pds/) \"New York City Taxi\". Este pipeline presenta una estrategia de [implementación canaria](https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/canary-deployment.html) con reversión en caso de error. La idea es poder entender cómo activar y monitorear el pipeline, inspeccionar el flujo de trabajo de entrenamiento, usar model monitor para configurar alertas y crear una implementación canary.\r\n",
    "\r\n",
    "### Contenido\r\n",
    "\r\n",
    "Este notebook contiene las siguientes secciones:\r\n",
    "\r\n",
    "1. [Data Prep](#Data-Prep)\r\n",
    "2. [Build](#Build)\r\n",
    "3. [Train Model](#Train-Model)\r\n",
    "4. [Deploy Dev](#Deploy-Dev)\r\n",
    "5. [Deploy Prod](#Deploy-Prod)\r\n",
    "6. [Monitor](#Monitor)\r\n",
    "6. [Cleanup](#Cleanup)\r\n",
    "\r\n",
    "### Arquitectura\r\n",
    "\r\n",
    "El diagrama de arquitectura a continuación muestra todo el pipeline de MLOps a un alto nivel.\r\n",
    "\r\n",
    "Usaremos la plantilla de CloudFormation proporcionada en este repositorio (`pipeline.yml`) para crear una demo en su propia cuenta de AWS. CloudFormation implementa varios recursos:\r\n",
    "   \r\n",
    "1. A customer-managed encryption key in in Amazon KMS for encrypting data and artifacts.\r\n",
    "1. A secret in Amazon Secrets Manager to securely store your GitHub Access Token.\r\n",
    "1. Several AWS IAM roles so CloudFormation, SageMaker, and other AWS services can perform actions in your AWS account, following the principle of [least privilege](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege)⇗.\r\n",
    "1. A messaging service in Amazon SNS to notify you when CodeDeploy has successfully deployed the API, and to receive alerts for retraining and drift detection (signing up for these notifications is optional).\r\n",
    "1. Two Amazon CloudWatch event rules: one which schedules the pipeline to run every month, and one which triggers the pipeline to run when SageMaker Model Monitor detects certain metrics.\r\n",
    "1. An Amazon SageMaker Jupyter notebook with this workshop content pre-loaded.\r\n",
    "1. An Amazon S3 bucket for storing model artifacts.\r\n",
    "1. An AWS CodePipeline instance with several pre-defined stages. \r\n",
    "\r\n",
    "Take a moment to look at all of these resources now deployed in your account. \r\n",
    "\r\n",
    "![MLOps pipeline architecture](../docs/mlops-architecture.png)\r\n",
    "\r\n",
    "En este notebook, trabajaremos a través de una instancia de CodePipeline creada por la plantilla de CloudFormation. Tiene varias etapas:\r\n",
    "\r\n",
    "1. **Source** - The pipeline is already configured with two sources. If you upload a new dataset to a specific location in the S3 data bucket, this will trigger the pipeline to run. The Git source can be GitHub, or CodeCommit if you don’t supply your access token. If you commit new code to your repository, this will trigger the pipeline to run. El pipeline ya está configurado con dos fuentes. Si carga un nuevo conjunto de datos en una ubicación específica en el bucket de S3, esto activará la ejecución del pipeline. La fuente de Git puede ser GitHub o CodeCommit si no proporciona su token de acceso. Si envías código nuevo a tu repositorio, esto activará la ejecución del pipeline.\r\n",
    "1. **Build** - En esta etapa, CodeBuild configurado por el archivo `model / buildspec.yml` ejecutará` model / run.py` para generar plantillas de AWS CloudFormation para crear un Step Function (incluidos los recursos personalizados de AWS Lambda) y las plantillas de implementación utilizadas en las siguientes etapas según los conjuntos de datos y los hiperparámetros especificados para esta ejecución del pipeline.\r\n",
    "1. **Train** The Step Functions workflow created in the Build stage is run in this stage. The workflow creates a baseline for the model monitor using a SageMaker processing job, and trains an XGBoost model on the taxi ride dataset using a SageMaker training job.\r\n",
    "1. **Deploy Dev** In this stage, a CloudFormation template created in the build stage (from `assets/deploy-model-dev.yml`) deploys a dev endpoint. This will allow you to run tests on the model and decide if the model is of sufficient quality to deploy into production.\r\n",
    "1. **Deploy Production** The final stage of the pipeline is the only stage which does not run automatically as soon as the previous stage is complete. It waits for a user to manually approve the model which was previously deployed to dev. As soon as the model is approved, a CloudFormation template (packaged from `assets/deploy-model-prod.yml` to include the Lambda functions saved and uploaded as ZIP files in S3) deploys the production endpoint. It configures autoscaling and enables data capture. It creates a model monitoring schedule and sets CloudWatch alarms for certain metrics. It also sets up an AWS CodeDeploy instance which deploys a set of AWS Lambda functions and an Amazon API Gateway to sit in front of the SageMaker endpoint. This stage can make use of canary deployment to safely switch from an old model to a new model. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Importamos las librerías necesarias\r\n",
    "import sys\r\n",
    "!{sys.executable} -m pip install --upgrade pip\r\n",
    "!{sys.executable} -m pip install -qU awscli boto3 \"sagemaker>=2.1.0<3\" tqdm\r\n",
    "!{sys.executable} -m pip install -qU \"stepfunctions==2.0.0\"\r\n",
    "!{sys.executable} -m pip show sagemaker stepfunctions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podría ser necesario reiniciar el kernel de Sagemaker para continuar."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Prep\r\n",
    "\r\n",
    "En esta sección del cuaderno, descargaremos el dataset disponible públicamente como preparación para cargarlo en S3.\r\n",
    "\r\n",
    "### Descargar Dataset\r\n",
    "\r\n",
    "Primero, descargamos una muestra del [dataset](https://registry.opendata.aws/nyc-tlc-trip-records-pds/)⇗. This dataset contains information on trips taken by taxis and for-hire vehicles in New York City, including pick-up and drop-off times and locations, fares, distance traveled, and more. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!aws s3 cp 's3://nyc-tlc/trip data/green_tripdata_2018-02.csv' 'nyc-tlc.csv'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cargamos el dataset en un dataframe de pandas, teniendo cuidado de parsear correctamente las fechas."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "parse_dates= ['lpep_dropoff_datetime', 'lpep_pickup_datetime']\r\n",
    "trip_df = pd.read_csv('nyc-tlc.csv', parse_dates=parse_dates)\r\n",
    "\r\n",
    "trip_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data manipulation\r\n",
    "\r\n",
    "En lugar de usar las fechas y horas de recojo y llegada, usaremos estos features para calcular el tiempo total del viaje en minutos, los cuáles serám fáciles de trabajar con nuestor modelo."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trip_df['duration_minutes'] = (trip_df['lpep_dropoff_datetime'] - trip_df['lpep_pickup_datetime']).dt.seconds/60"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "El dataset contiene un monton de columnas que no necesitamos, vamos a seleccionar una muestra de las columnas para nuestro modelo de ML. Mantenemos sólo `total_amount` (fare), `duration_minutes`, `passenger_count`, y `trip_distance`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cols = ['total_amount', 'duration_minutes', 'passenger_count', 'trip_distance']\r\n",
    "data_df = trip_df[cols]\r\n",
    "print(data_df.shape)\r\n",
    "data_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generamos algunas estadísticas del dataset para entender la calidad."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_df.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "La tabla de arriba muestra algunos outliers, por ejemplo -400 o 2626 como tarifa, o 0 pasajeros. Hay muchos métodos inteligemtes para identificar y remover outliers pero la limpieza de datos no es el foco de este notebook, asi que solo removemos los outliers configurando valores mínimos y máximos los cuáles parecen más razonables. Eliminando los outliers obtenermos 754,671 filas en el dataset final."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_df = data_df[(data_df.total_amount > 0) & (data_df.total_amount < 200) & \r\n",
    "                  (data_df.duration_minutes > 0) & (data_df.duration_minutes < 120) & \r\n",
    "                  (data_df.trip_distance > 0) & (data_df.trip_distance < 121) & \r\n",
    "                  (data_df.passenger_count > 0)].dropna()\r\n",
    "print(data_df.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data visualization\n",
    "\n",
    "Since this notebook will build a regression model for the taxi data, it's a good idea to check if there is any correlation between the variables in our data. Use scatter plots on a sample of the data to compare trip distance with duration in minutes, and total amount (fare) with duration in minutes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import seaborn as sns \r\n",
    "\r\n",
    "sample_df = data_df.sample(1000)\r\n",
    "sns.scatterplot(data=sample_df, x='duration_minutes', y='trip_distance')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.scatterplot(data=sample_df, x='duration_minutes', y='total_amount')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "These scatter plots look fine and show at least some correlation between our variables. \n",
    "\n",
    "### Data splitting and saving\n",
    "\n",
    "We are now ready to split the dataset into train, validation, and test sets. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "train_df, val_df = train_test_split(data_df, test_size=0.20, random_state=42)\r\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.05, random_state=42)\r\n",
    "\r\n",
    "# Reset the index for our test dataframe\r\n",
    "test_df.reset_index(inplace=True, drop=True)\r\n",
    "\r\n",
    "print('Size of\\n train: {},\\n val: {},\\n test: {} '.format(train_df.shape[0], val_df.shape[0], test_df.shape[0]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the train, validation, and test files as CSV locally on this notebook instance. Notice that you save the train file twice - once as the training data file and once as the baseline data file. The baseline data file will be used by [SageMaker Model Monitor](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html)⇗ to detect data drift. Data drift occurs when the statistical nature of the data that your model receives while in production drifts away from the nature of the baseline data it was trained on, which means the model begins to lose accuracy in its predictions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_cols = ['total_amount', 'duration_minutes','passenger_count','trip_distance']\r\n",
    "train_df.to_csv('train.csv', index=False, header=False)\r\n",
    "val_df.to_csv('validation.csv', index=False, header=False)\r\n",
    "test_df.to_csv('test.csv', index=False, header=False)\r\n",
    "\r\n",
    "# Save test and baseline with headers\r\n",
    "train_df.to_csv('baseline.csv', index=False, header=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now upload these CSV files to your default SageMaker S3 bucket. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sagemaker\r\n",
    "\r\n",
    "# Get the session and default bucket\r\n",
    "session = sagemaker.session.Session()\r\n",
    "bucket = session.default_bucket()\r\n",
    "\r\n",
    "# Specify data prefix and version\r\n",
    "prefix = 'nyc-tlc/v1'\r\n",
    "\r\n",
    "s3_train_uri = session.upload_data('train.csv', bucket, prefix + '/data/training')\r\n",
    "s3_val_uri = session.upload_data('validation.csv', bucket, prefix + '/data/validation')\r\n",
    "s3_test_uri = session.upload_data('test.csv', bucket, prefix + '/data/test')\r\n",
    "s3_baseline_uri = session.upload_data('baseline.csv', bucket, prefix + '/data/baseline')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You will use the datasets which you have prepared and saved in this section to trigger the pipeline to train and deploy a model in the next section."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build\r\n",
    "\r\n",
    "Si vamos a CodePipeline, notaremos que la etapa \"Source\" está inicialmente en un estado \"Fallido\". Esto sucede porque el conjunto de datos, que es una de las fuentes que pueden desencadenar el pipeline, aún no se ha cargado en la ubicación S3 esperada.\r\n",
    "\r\n",
    "![Failed code pipeline](../docs/pipeline_failed.png)\r\n",
    "\r\n",
    "### Trigger Build\r\n",
    " \r\n",
    "En esta sección, iniciaremos un pipeline de implementación y compilación de modelos empaquetando los datasets que preparamos en la sección anterior y cargándolos en la ubicación de origen de S3 que activa la instancia de CodePipeline creada.\r\n",
    "\r\n",
    "Primero, importamos algunas librerías y cargaremos algunas variables de entorno que se van a necesitar. Estas variables de entorno se han establecido a través de un script de [lifecycle configuration](https://docs.aws.amazon.com/sagemaker/latest/dg/notebook-lifecycle-config.html)⇗ adjunto a este notebook."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import boto3\r\n",
    "from botocore.exceptions import ClientError\r\n",
    "import os\r\n",
    "import time\r\n",
    "\r\n",
    "region = boto3.Session().region_name\r\n",
    "artifact_bucket = os.environ['ARTIFACT_BUCKET']\r\n",
    "pipeline_name = os.environ['PIPELINE_NAME']\r\n",
    "model_name = os.environ['MODEL_NAME']\r\n",
    "workflow_pipeline_arn = os.environ['WORKFLOW_PIPELINE_ARN']\r\n",
    "\r\n",
    "print('region: {}'.format(region))\r\n",
    "print('artifact bucket: {}'.format(artifact_bucket))\r\n",
    "print('pipeline: {}'.format(pipeline_name))\r\n",
    "print('model name: {}'.format(model_name))\r\n",
    "print('workflow: {}'.format(workflow_pipeline_arn))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the AWS CodePipeline [documentation](https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-simple-s3.html)⇗:\r\n",
    "\r\n",
    "> Cuando Amazon S3 es el proveedor de origen de un pipeline, puede comprimir un archivo o archivos de origen en un solo .zip y cargar el .zip en el backet de origen. También puede cargar un solo archivo descomprimido; sin embargo, las acciones posteriores que esperan un archivo .zip fallarán.\r\n",
    "\r\n",
    "Para entrenar un modelo, necesitaremos varios datasets (entrenamiento, validación y prueba) junto con un archivo que especifica los hiperparámetros. En este ejemplo, crearemos un archivo JSON que contiene las ubicaciones de los datasets en S3 y un archivo JSON que contiene los valores de los hiperparámetros. Luego, comprimiremos ambos archivos en un zip que se utilizará como entrada para la ejecución del pipeline."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from io import BytesIO\r\n",
    "import zipfile\r\n",
    "import json\r\n",
    "\r\n",
    "input_data = {\r\n",
    "    'TrainingUri': s3_train_uri,\r\n",
    "    'ValidationUri': s3_val_uri,\r\n",
    "    'TestUri': s3_test_uri,\r\n",
    "    'BaselineUri': s3_baseline_uri\r\n",
    "}\r\n",
    "\r\n",
    "hyperparameters = {\r\n",
    "    'num_round': 50\r\n",
    "}\r\n",
    "\r\n",
    "zip_buffer = BytesIO()\r\n",
    "with zipfile.ZipFile(zip_buffer, 'a') as zf:\r\n",
    "    zf.writestr('inputData.json', json.dumps(input_data))\r\n",
    "    zf.writestr('hyperparameters.json', json.dumps(hyperparameters))\r\n",
    "zip_buffer.seek(0)\r\n",
    "\r\n",
    "data_source_key = '{}/data-source.zip'.format(pipeline_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now upload the zip package to your artifact S3 bucket - this action will trigger the pipeline to train and deploy a model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "s3 = boto3.client('s3')\r\n",
    "s3.put_object(Bucket=artifact_bucket, Key=data_source_key, Body=bytearray(zip_buffer.read()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Click the link below to open the AWS console at the Code Pipeline if you don't have it open in another tab.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    Tip: You may need to wait a minute to see the DataSource stage turn green. The page will refresh automatically.\n",
    "</div>\n",
    "\n",
    "![Source Green](../docs/datasource-after.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from IPython.core.display import HTML\r\n",
    "\r\n",
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/codesuite/codepipeline/pipelines/{1}/view?region={0}\">Code Pipeline</a>'.format(region, pipeline_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspect Build Logs\n",
    "\n",
    "Once the build stage is running, you will see the AWS CodeBuild job turn blue with a status of **In progress**.\n",
    "\n",
    "![Failed code pipeline](../docs/codebuild-inprogress.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can click on the **Details** link displayed in the CodePipeline UI or click the link below to jump directly to the CodeBuild logs.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    Tip: You may need to wait a few seconds for the pipeline to transition into the active (blue) state and for the build to start.\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "codepipeline = boto3.client('codepipeline')\r\n",
    "\r\n",
    "def get_pipeline_stage(pipeline_name, stage_name):\r\n",
    "    response = codepipeline.get_pipeline_state(name=pipeline_name)\r\n",
    "    for stage in response['stageStates']:\r\n",
    "        if stage['stageName'] == stage_name:\r\n",
    "            return stage\r\n",
    "\r\n",
    "# Get last execution id\r\n",
    "build_stage = get_pipeline_stage(pipeline_name, 'Build')    \r\n",
    "if not 'latestExecution' in build_stage:\r\n",
    "    raise(Exception('Please wait.  Build not started'))\r\n",
    "\r\n",
    "build_url = build_stage['actionStates'][0]['latestExecution']['externalExecutionUrl']\r\n",
    "\r\n",
    "# Out a link to the code build logs\r\n",
    "HTML('<a target=\"_blank\" href=\"{0}\">Code Build Logs</a>'.format(build_url))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The AWS CodeBuild process is responsible for creating a number of AWS CloudFormation templates which we will explore in more detail in the next section.  Two of these templates are used to set up the **Train** step by creating the AWS Step Functions worklow and the custom AWS Lambda functions used within this workflow."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Model\n",
    "\n",
    "### Inspect Training Job\n",
    "\n",
    "Wait until the pipeline has started running the Train step (see screenshot) before continuing with the next cells in this notebook. \n",
    "\n",
    "![Training in progress](../docs/train-in-progress.png)\n",
    "\n",
    "When the pipeline has started running the train step, you can click on the **Details** link displayed in the CodePipeline UI (see screenshot above) to view the Step Functions workflow which is running the training job. \n",
    "\n",
    "Alternatively, you can click on the Workflow link from the cell output below once it's available."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from stepfunctions.workflow import Workflow\r\n",
    "while True:\r\n",
    "    try:\r\n",
    "        workflow = Workflow.attach(workflow_pipeline_arn)\r\n",
    "        break\r\n",
    "    except ClientError as e:\r\n",
    "        print(e.response[\"Error\"][\"Message\"])\r\n",
    "    time.sleep(10)\r\n",
    "\r\n",
    "workflow"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or simply run the cell below to display the Step Functions workflow, and re-run it after a few minutes to see the progress."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "executions = workflow.list_executions()\r\n",
    "if not executions:\r\n",
    "    raise(Exception('Please wait.  Training not started'))\r\n",
    "    \r\n",
    "executions[0].render_progress()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Revisamos el script de Build\r\n",
    "\r\n",
    "Mientras esperamos a que se complete el trabajo de entrenamiento, revisemos el código `run.py` que fue utilizado por el proceso de AWS CodeBuild.\r\n",
    "\r\n",
    "Este script toma todos los parámetros de entrada, incluidas las ubicaciones de los datasets y los hiperparámetros que guardó en archivos JSON anteriormente en este notebook, y los usa para generar las plantillas que el pipeline necesita para ejecutar el job de entrenamiento. * No * crea la instancia de Step Functions real; solo genera las plantillas que definen el workflow de Step Functions, así como las plantillas de entrada de CloudFormation que CodePipeline usa para crear la instancia de Step Functions.\r\n",
    "\r\n",
    "Paso a paso, el script hace lo siguiente:\r\n",
    "\r\n",
    "1. Recopila todos los parámetros de entrada que necesita para generar las plantillas. Esto incluye información sobre el contenedor de entorno necesario para ejecutar el job de entrenamiento, las ubicaciones de los datos de entrada y salida, los roles de IAM necesarios para varios componentes, las claves de cifrado y más. Luego configura algunos parámetros básicos como la región de AWS y los nombres de las funciones.\r\n",
    "1. Si los parámetros de entrada especifican un contenedor de entorno almacenado en ECR, obtiene ese contenedor. De lo contrario, obtiene el URI del contenedor del entorno administrado de AWS necesario para el job de entrenamiento.\r\n",
    "1. Lee el archivo JSON de datos de entrada que generó anteriormente en este notebook (y que se incluyó en la fuente zip para el pipeline), obteniendo así las ubicaciones de entrenamiento, validación y los archivos de datos baseline. Luego, formatea más parámetros que se necesitarán más adelante en el script, incluidos los ID de versión y las ubicaciones de los datos de salida.\r\n",
    "1. Lee el archivo JSON de hiperparámetro que generó anteriormente en este notebook.\r\n",
    "1. Define el workflow de Step Functions, comenzando con el esquema de entrada, seguido de cada paso del flujo de trabajo (es decir, crear experimento, trabajo de referencia, trabajo de entrenamiento) y finalmente combina esos pasos en un gráfico de flujo de trabajo.\r\n",
    "1. El gráfico del flujo de trabajo se guarda en un archivo, junto con un archivo que contiene todos los parámetros de entrada guardados de acuerdo con el esquema definido en el flujo de trabajo.\r\n",
    "1. Guarda los parámetros en un archivo que CloudFormation utilizará para crear una instancia de workflow de Step Functions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pygmentize ../model/run.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Customize Workflow (Optional)\n",
    "\n",
    "If you are interested in customising the workflow used in the Build Script, store the `input_data` to be used within the local [workflow.ipynb](workflow.ipynb) notebook. The workflow notebook can be used to experiment with the Step Functions workflow and training job definitions for your model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%store input_data"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Analytics\n",
    "\n",
    "Once the training and baseline jobs are complete (meaning they are displayed in a green color in the Step Functions workflow, this takes around 5 minutes), you can inspect the experiment metrics. The code below will display all experiments in a table. Note that the baseline processing job won't have RMSE metrics - it calculates metrics based on the training data, but does not train a machine learning model. \n",
    "\n",
    "You will [explore the baseline](#Explore-Baseline) results later in this notebook. <a id=\"validation-results\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sagemaker import analytics\r\n",
    "experiment_name = 'mlops-{}'.format(model_name)\r\n",
    "model_analytics = analytics.ExperimentAnalytics(experiment_name=experiment_name)\r\n",
    "analytics_df = model_analytics.dataframe()\r\n",
    "\r\n",
    "if (analytics_df.shape[0] == 0):\r\n",
    "    raise(Exception('Please wait.  No training or baseline jobs'))\r\n",
    "\r\n",
    "pd.set_option('display.max_colwidth', 100) # Increase column width to show full copmontent name\r\n",
    "cols = ['TrialComponentName', 'DisplayName', 'SageMaker.InstanceType', \r\n",
    "        'train:rmse - Last', 'validation:rmse - Last'] # return the last rmse for training and validation\r\n",
    "analytics_df[analytics_df.columns & cols].head(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deploy Dev\r\n",
    "\r\n",
    "### Test Dev Deployment\r\n",
    "\r\n",
    "Cuando el pipeline ha terminado de entrenar un modelo, pasa automáticamente al siguiente paso, donde el modelo se implementa como un endpoint de SageMaker. Este endpoint es parte de la implementación de desarrollo, por lo tanto, en esta sección, ejecutaremos algunas pruebas en el endpoint para decidir si deseamos implementar este modelo en producción.\r\n",
    "\r\n",
    "Primero, ejecutamos la celda a continuación para buscar el nombre del SageMaker Endpoint."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "codepipeline = boto3.client('codepipeline')\r\n",
    "\r\n",
    "deploy_dev = get_pipeline_stage(pipeline_name, 'DeployDev')\r\n",
    "if not 'latestExecution' in deploy_dev:\r\n",
    "    raise(Exception('Please wait.  Deploy dev not started'))\r\n",
    "    \r\n",
    "execution_id = deploy_dev['latestExecution']['pipelineExecutionId']\r\n",
    "dev_endpoint_name = 'mlops-{}-dev-{}'.format(model_name, execution_id)\r\n",
    "\r\n",
    "print('endpoint name: {}'.format(dev_endpoint_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Si pasamos por la sección anterior muy rápidamente, deberemos esperar hasta que el endpoint de desarrollo se haya implementado con éxito y el pipeline esté esperando la aprobación para implementar en producción (ver captura de pantalla). SageMaker puede tardar hasta 10 minutos en crear un punto final.\r\n",
    "\r\n",
    "![Deploying dev endpoint in code pipeline](../docs/dev-deploy-ready.png)\r\n",
    "\r\n",
    "Alternativamente, ejecutaremos el código a continuación para verificar el estado del endpoint. Esperamos hasta que el estado sea 'InService'."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sm = boto3.client('sagemaker')\r\n",
    "\r\n",
    "while True:\r\n",
    "    try:\r\n",
    "        response = sm.describe_endpoint(EndpointName=dev_endpoint_name)\r\n",
    "        print(\"Endpoint status: {}\".format(response['EndpointStatus']))\r\n",
    "        if response['EndpointStatus'] == 'InService':\r\n",
    "            break\r\n",
    "    except ClientError as e:\r\n",
    "        print(e.response[\"Error\"][\"Message\"])\r\n",
    "    time.sleep(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora que el endpoint está listo, escribamos un código para ejecutar los datos de prueba (que separamos aneriormente del conjunto de datos y se guardó en un archivo al comienzo de este notebook) a través del endpoint de inferencia. El siguiente código es compatible con la v1 y la v2 del SDK de SageMaker, pero recomendamos utilizar la v2 del SDK en todos sus proyectos futuros."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "try:\r\n",
    "    # Support SageMaker v2 SDK: https://sagemaker.readthedocs.io/en/stable/v2.html\r\n",
    "    from sagemaker.predictor import Predictor\r\n",
    "    from sagemaker.serializers import CSVSerializer\r\n",
    "    def get_predictor(endpoint_name):\r\n",
    "        xgb_predictor = Predictor(endpoint_name)\r\n",
    "        xgb_predictor.serializer = CSVSerializer()\r\n",
    "        return xgb_predictor\r\n",
    "except:\r\n",
    "    # Fallback to SageMaker v1.70 SDK\r\n",
    "    from sagemaker.predictor import RealTimePredictor, csv_serializer\r\n",
    "    def get_predictor(endpoint_name):\r\n",
    "        xgb_predictor = RealTimePredictor(endpoint_name)\r\n",
    "        xgb_predictor.content_type = 'text/csv'\r\n",
    "        xgb_predictor.serializer = csv_serializer\r\n",
    "        return xgb_predictor\r\n",
    "\r\n",
    "def predict(predictor, data, rows=500):\r\n",
    "    split_array = np.array_split(data, round(data.shape[0] / float(rows)))\r\n",
    "    predictions = ''\r\n",
    "    for array in tqdm(split_array):\r\n",
    "        predictions = ','.join([predictions, predictor.predict(array).decode('utf-8')])\r\n",
    "    return np.fromstring(predictions[1:], sep=',')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now use the `predict` function, which was defined in the code above, to run the test data through the endpoint and generate the predictions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dev_predictor = get_predictor(dev_endpoint_name)\r\n",
    "predictions = predict(dev_predictor, test_df[test_df.columns[1:]].values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, load the predictions into a data frame, and join it with your test data. Then, calculate absolute error as the difference between the actual taxi fare and the predicted taxi fare. Display the results in a table, sorted by the highest absolute error values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred_df = pd.DataFrame({'total_amount_predictions': predictions })\r\n",
    "pred_df = test_df.join(pred_df) # Join on all\r\n",
    "pred_df['error'] = abs(pred_df['total_amount']-pred_df['total_amount_predictions'])\r\n",
    "\r\n",
    "pred_df.sort_values('error', ascending=False).head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From this table, we note that some short trip distances have large errors because the low predicted fare does not match the high actual fare. This could be the result of a generous tip which we haven't included in this dataset.\n",
    "\n",
    "You can also analyze the results by plotting the absolute error to visualize outliers. In this graph, we see that most of the outliers are cases where the model predicted a much lower fare than the actual fare. There are only a few outliers where the model predicted a higher fare than the actual fare."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.scatterplot(data=pred_df, x='total_amount_predictions', y='total_amount', hue='error')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you want one overall measure of quality for the model, you can calculate the root mean square error (RMSE) for the predicted fares compared to the actual fares. Compare this to the [results calculated on the validation set](#validation-results) at the end of the 'Inspect Training Job' section."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from math import sqrt\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "\r\n",
    "def rmse(pred_df):\r\n",
    "    return sqrt(mean_squared_error(pred_df['total_amount'], pred_df['total_amount_predictions']))\r\n",
    "\r\n",
    "print('RMSE: {}'.format(rmse(pred_df)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deploy Prod\n",
    "\n",
    "### Approve Deployment to Production\n",
    "\n",
    "If you are happy with the results of the model, you can go ahead and approve the model to be deployed into production. You can do so by clicking the **Review** button in the CodePipeline UI, leaving a comment to explain why you approve this model, and clicking on **Approve**. \n",
    "\n",
    "Alternatively, you can create a Jupyter widget which (when enabled) allows you to comment and approve the model directly from this notebook. Run the cell below to see this in action."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import ipywidgets as widgets\r\n",
    "\r\n",
    "def on_click(obj):\r\n",
    "    result = { 'summary': approval_text.value, 'status': obj.description }\r\n",
    "    response = codepipeline.put_approval_result(\r\n",
    "      pipelineName=pipeline_name,\r\n",
    "      stageName='DeployDev',\r\n",
    "      actionName='ApproveDeploy',\r\n",
    "      result=result,\r\n",
    "      token=approval_action['token']\r\n",
    "    )\r\n",
    "    button_box.close()\r\n",
    "    print(result)\r\n",
    "    \r\n",
    "# Create the widget if we are ready for approval\r\n",
    "deploy_dev = get_pipeline_stage(pipeline_name, 'DeployDev')\r\n",
    "if not 'latestExecution' in deploy_dev['actionStates'][-1]:\r\n",
    "    raise(Exception('Please wait.  Deploy dev not complete'))\r\n",
    "\r\n",
    "approval_action = deploy_dev['actionStates'][-1]['latestExecution']\r\n",
    "if approval_action['status'] == 'Succeeded':\r\n",
    "    print('Dev approved: {}'.format(approval_action['summary']))\r\n",
    "elif 'token' in approval_action:\r\n",
    "    approval_text = widgets.Text(placeholder='Optional approval message')   \r\n",
    "    approve_btn = widgets.Button(description=\"Approved\", button_style='success', icon='check')\r\n",
    "    reject_btn = widgets.Button(description=\"Rejected\", button_style='danger', icon='close')\r\n",
    "    approve_btn.on_click(on_click)\r\n",
    "    reject_btn.on_click(on_click)\r\n",
    "    button_box = widgets.HBox([approval_text, approve_btn, reject_btn])\r\n",
    "    display(button_box)\r\n",
    "else:\r\n",
    "    raise(Exception('Please wait. No dev approval'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Production Deployment\r\n",
    "\r\n",
    "Aproximadamente un minuto después de aprobar la implementación del modelo, debería ver que el pipeline comienza en el paso final: implementar el modelo en producción. En esta sección, comprobaremos el estado de implementación y probaremos el endpoint de producción después de que se haya implementado.\r\n",
    "\r\n",
    "![Deploy production endpoint in code pipeline](../docs/deploy-production.png)\r\n",
    "\r\n",
    "Este paso del pipeline utiliza CloudFormation para implementar una serie de recursos en su nombre. En particular, crea:\r\n",
    "\r\n",
    "1. Endpoint de sagemaker listo para producción , con [data capture](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-capture.html)⇗  (used by SageMaker Model Monitor) y [autoscaling](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html)⇗ enabled.\r\n",
    "1. Un [model monitoring schedule](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-scheduling.html)⇗ que envía los resultados a de CloudWatch Metrics, junto con un [CloudWatch Alarm](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html)⇗ que le notificará cuando ocurra una infracción.\r\n",
    "1. Una instancia de CodeDeploy que crea una aplicación simple mediante la implementación de API Gateway, tres funciones de Lambda y una alarma para notificar el éxito o el fracaso de esta implementación. El código para las funciones de Lambda se puede encontrar en `api/app.py`,` api/pre_traffic_hook.py` y `api/ post_traffic_hook.py`. Estas funciones actualizan el endpoint para permitir la captura de datos, formatear y enviar el tráfico entrante al endpoint de SageMaker y capturar los registros de datos.\r\n",
    "\r\n",
    "![Components of production deployment](../docs/cloud-formation.png)\r\n",
    "\r\n",
    "Veamos cómo avanza la implementación. Utilice el siguiente código para obtener el ID de ejecución del paso de implementación. Luego, genere una tabla que enumere los recursos creados por el stack de CloudFormation y su estado de creación. Puede volver a ejecutar la celda después de unos minutos para ver cómo avanzan los pasos."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "deploy_prd = get_pipeline_stage(pipeline_name, 'DeployPrd')\r\n",
    "if not 'latestExecution' in deploy_prd or not 'latestExecution' in deploy_prd['actionStates'][0]:\r\n",
    "    raise(Exception('Please wait.  Deploy prd not started'))\r\n",
    "    \r\n",
    "execution_id = deploy_prd['latestExecution']['pipelineExecutionId']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datetime import datetime, timedelta\r\n",
    "from dateutil.tz import tzlocal\r\n",
    "\r\n",
    "def get_event_dataframe(events):\r\n",
    "    stack_cols = ['LogicalResourceId', 'ResourceStatus', 'ResourceStatusReason', 'Timestamp']\r\n",
    "    stack_event_df = pd.DataFrame(events)[stack_cols].fillna('')\r\n",
    "    stack_event_df['TimeAgo'] = (datetime.now(tzlocal())-stack_event_df['Timestamp'])\r\n",
    "    return stack_event_df.drop('Timestamp', axis=1)\r\n",
    "\r\n",
    "cfn = boto3.client('cloudformation')\r\n",
    "\r\n",
    "stack_name = stack_name='{}-deploy-prd'.format(pipeline_name)\r\n",
    "print('stack name: {}'.format(stack_name))\r\n",
    "\r\n",
    "# Get latest stack events\r\n",
    "while True:\r\n",
    "    try:\r\n",
    "        response = cfn.describe_stack_events(StackName=stack_name)\r\n",
    "        break\r\n",
    "    except ClientError as e:\r\n",
    "        print(e.response[\"Error\"][\"Message\"])\r\n",
    "    time.sleep(10)\r\n",
    "    \r\n",
    "get_event_dataframe(response['StackEvents']).head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "El recurso que más nos interesa es el endpoint. Esto demora una promedio de 10 minutos en implementarse. Mientras tanto, puede echar un vistazo al código Python utilizado para la aplicación. \r\n",
    "\r\n",
    "`App.py` es el principal punto de entrada que invoca el endpoint de Amazon SageMaker. Devuelve resultados junto con un encabezado personalizado para el endpoint que invocamos."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pygmentize ../api/app.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `pre_traffic_hook.py` lambda is invoked prior to deployment and confirms the endpoint has data capture enabled."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pygmentize ../api/pre_traffic_hook.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `post_traffic_hook.py` lambda is invoked to perform any final checks, in this case to verify that we have received log data from data capature."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pygmentize ../api/post_traffic_hook.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the code below to fetch the name of the endpoint, then run a loop to wait for the endpoint to be fully deployed. You need the status to be 'InService'."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prd_endpoint_name='mlops-{}-prd-{}'.format(model_name, execution_id)\r\n",
    "print('prod endpoint: {}'.format(prd_endpoint_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sm = boto3.client('sagemaker')\r\n",
    "\r\n",
    "while True:\r\n",
    "    try:\r\n",
    "        response = sm.describe_endpoint(EndpointName=prd_endpoint_name)\r\n",
    "        print(\"Endpoint status: {}\".format(response['EndpointStatus']))\r\n",
    "        # Wait until the endpoint is in service with data capture enabled\r\n",
    "        if response['EndpointStatus'] == 'InService' \\\r\n",
    "            and 'DataCaptureConfig' in response \\\r\n",
    "            and response['DataCaptureConfig']['EnableCapture']:\r\n",
    "            break\r\n",
    "    except ClientError as e:\r\n",
    "        print(e.response[\"Error\"][\"Message\"])\r\n",
    "    time.sleep(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cuando el estado del endpoint es 'InService', puede continuar. Anteriormente en este cuaderno, creamos un código para enviar datos al endpoint de desarrollo. Reutilice este código ahora para enviar una muestra de los datos de prueba al endpoint de producción. Dado que la captura de datos está habilitada en este punto final, desea enviar registros individuales a la vez, de modo que el monitor del modelo pueda asignar estos registros a la línea de base.\r\n",
    "\r\n",
    "Más tarde haremos uso de [inspect the model monitor](#Inspect-Model-Monitor). For now, just check if you can send data to the endpoint and receive predictions in return. Por ahora, solo verifique si puede enviar datos al punto final y recibir predicciones a cambio."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prd_predictor = get_predictor(prd_endpoint_name)\r\n",
    "sample_values = test_df[test_df.columns[1:]].sample(100).values\r\n",
    "predictions = predict(prd_predictor, sample_values, rows=1)\r\n",
    "predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test REST API\r\n",
    "\r\n",
    "Aunque ya probó el endpoint de SageMaker en la sección anterior, también es una buena idea probar la aplicación creada con API Gateway.\r\n",
    "\r\n",
    "![Traffic shift between endpoints](../docs/lambda-deploy-create.png)\r\n",
    "\r\n",
    "Dar clic el enlace a continuación para abrir la implementación en Lambda, donde puede ver las implementaciones en progreso y completadas. También puede hacer clic para expandir la ** plantilla SAM ** para ver la plantilla de CloudFormation empaquetada utilizada en la implementación."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/lambda/home?region={0}#/applications/{1}-deploy-prd?tab=deploy\">Lambda Deployment</a>'.format(region, model_name))"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ejecute el siguiente código para confirmar que el punto final está en servicio. Se completará una vez que la API REST esté disponible."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_stack_status(stack_name):\r\n",
    "    response = cfn.describe_stacks(StackName=stack_name)\r\n",
    "    if response['Stacks']:\r\n",
    "        stack = response['Stacks'][0]\r\n",
    "        outputs = None\r\n",
    "        if 'Outputs' in stack:\r\n",
    "            outputs = dict([(o['OutputKey'], o['OutputValue']) for o in stack['Outputs']])\r\n",
    "        return stack['StackStatus'], outputs \r\n",
    "\r\n",
    "outputs = None\r\n",
    "while True:\r\n",
    "    try:\r\n",
    "        status, outputs = get_stack_status(stack_name)\r\n",
    "        response = sm.describe_endpoint(EndpointName=prd_endpoint_name)\r\n",
    "        print(\"Endpoint status: {}\".format(response['EndpointStatus']))\r\n",
    "        if outputs:\r\n",
    "            break\r\n",
    "        elif status.endswith('FAILED'):\r\n",
    "            raise(Exception('Stack status: {}'.format(status)))\r\n",
    "    except ClientError as e:\r\n",
    "        print(e.response[\"Error\"][\"Message\"])\r\n",
    "    time.sleep(10)\r\n",
    "\r\n",
    "if outputs:\r\n",
    "    print('deployment application: {}'.format(outputs['DeploymentApplication']))\r\n",
    "    print('rest api: {}'.format(outputs['RestApi']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Si está realizando una actualización en su implementación de producción como resultado de ejecutar [Trigger Retraining](#Trigger-Retraining), podrá expandir la pestaña Implementación de Lambda para revelar los recursos. Haga clic en el enlace **ApiFunctionAliaslive** para ver la implementación de Lambda en curso.\r\n",
    "\r\n",
    "![Traffic shift between endpoints](../docs/lambda-deploy-update.png)\r\n",
    "\r\n",
    "Esta página se actualizará para enumerar los eventos de implementación. También tiene un enlace a la aplicación de implementación a la que puede acceder en la salida de la siguiente celda."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/codesuite/codedeploy/applications/{1}?region={0}\">CodeDeploy application</a>'.format(region, outputs['DeploymentApplication']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "CodeDeploy realizará una implementación canary y enviará el 10% del tráfico al nuevo endpoint durante un período de 5 minutos.\r\n",
    "\r\n",
    "![Traffic shift between endpoints](../docs/code-deploy.gif)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podemos invocar la API REST e inspeccionar los encabezados que se devuelven para ver qué endpoint estamos alcanzando. Ocasionalmente veremos que la celda a continuación muestra un endpoint diferente que se ajusta a la nueva versión una vez que se completa el stack."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "\r\n",
    "from urllib import request\r\n",
    "\r\n",
    "headers = {\"Content-type\": \"text/csv\"}\r\n",
    "payload = test_df[test_df.columns[1:]].head(1).to_csv(header=False, index=False).encode('utf-8')\r\n",
    "rest_api = outputs['RestApi']\r\n",
    "\r\n",
    "while True:\r\n",
    "    try:\r\n",
    "        resp = request.urlopen(request.Request(rest_api, data=payload, headers=headers))\r\n",
    "        print(\"Response code: %d: endpoint: %s\" % (resp.getcode(), resp.getheader('x-sagemaker-endpoint')))\r\n",
    "        status, outputs = get_stack_status(stack_name) \r\n",
    "        if status.endswith('COMPLETE'):\r\n",
    "            print('Deployment complete\\n')\r\n",
    "            break\r\n",
    "        elif status.endswith('FAILED'):\r\n",
    "            raise(Exception('Stack status: {}'.format(status)))\r\n",
    "    except ClientError as e:\r\n",
    "        print(e.response[\"Error\"][\"Message\"])\r\n",
    "    time.sleep(10)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Monitor\r\n",
    "\r\n",
    "### Inspect Model Monitor\r\n",
    "\r\n",
    "Cuando preparamos los conjuntos de datos para el entrenamiento de modelos al comienzo de este cuaderno, guardó un conjunto de datos de referencia (una copia del conjunto de datos del tren). Luego, cuando aprobó el modelo para la implementación en producción, la canalización configuró un SageMaker Endpoint con la captura de datos habilitada y un programa de monitoreo del modelo. En esta sección, observará más de cerca los resultados del monitor modelo.\r\n",
    "\r\n",
    "Para comenzar, obtenga el ID de ejecución de implementación de producción más reciente."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "deploy_prd = get_pipeline_stage(pipeline_name, 'DeployPrd')\r\n",
    "if not 'latestExecution' in deploy_prd:\r\n",
    "    raise(Exception('Please wait.  Deploy prod not complete'))\r\n",
    "    \r\n",
    "execution_id = deploy_prd['latestExecution']['pipelineExecutionId']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bajo el capó, el monitor modelo SageMaker se ejecuta en trabajos de procesamiento de SageMaker. Utilice el ID de ejecución para obtener los nombres del trabajo de procesamiento y la scheduling."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "processing_job_name='mlops-{}-pbl-{}'.format(model_name, execution_id)\r\n",
    "schedule_name='mlops-{}-pms'.format(model_name)\r\n",
    "\r\n",
    "print('processing job name: {}'.format(processing_job_name))\r\n",
    "print('schedule name: {}'.format(schedule_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Explore Baseline\r\n",
    "\r\n",
    "Ahora obtenga los resultados del baseline del trabajo de procesamiento. Esta celda generará una excepción si el trabajo de procesamiento no está completo; si eso sucede, espere varios minutos y vuelva a intentarlo. <a id=\"view-baseline-results\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sagemaker\r\n",
    "from sagemaker.model_monitor import BaseliningJob, MonitoringExecution\r\n",
    "from sagemaker.s3 import S3Downloader\r\n",
    "\r\n",
    "sagemaker_session = sagemaker.Session()\r\n",
    "baseline_job = BaseliningJob.from_processing_name(sagemaker_session, processing_job_name)\r\n",
    "status = baseline_job.describe()['ProcessingJobStatus']\r\n",
    "if status != 'Completed':\r\n",
    "    raise(Exception('Please wait. Processing job not complete, status: {}'.format(status)))\r\n",
    "    \r\n",
    "baseline_results_uri  = baseline_job.outputs[0].destination"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SageMaker model monitor generates two types of files. Take a look at the statistics file first. It calculates various statistics for each feature of the dataset, including the mean, standard deviation, minimum value, maximum value, and more. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import json\r\n",
    "\r\n",
    "baseline_statistics = baseline_job.baseline_statistics().body_dict\r\n",
    "schema_df = pd.json_normalize(baseline_statistics[\"features\"])\r\n",
    "schema_df[[\"name\", \"numerical_statistics.mean\", \"numerical_statistics.std_dev\",\r\n",
    "           \"numerical_statistics.min\", \"numerical_statistics.max\"]].head()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now look at the suggested [constraints files](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-constraints.html)⇗. As the name implies, these are constraints which SageMaker model monitor recommends. If the live data which is sent to your production SageMaker Endpoint violates these constraints, this indicates data drift, and model monitor can raise an alert to trigger retraining. Of course, you can set different constraints based on the statistics which you viewed previously."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "baseline_constraints = baseline_job.suggested_constraints().body_dict\r\n",
    "constraints_df = pd.json_normalize(baseline_constraints[\"features\"])\r\n",
    "constraints_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### View data capture\n",
    "\n",
    "When the \"Deploy Production\" stage of the MLOps pipeline deploys a SageMaker endpoint, it also enables data capture. This means the incoming requests to the endpoint, as well as the results from the ML model, are stored in an S3 location. Model monitor can analyze this data and compare it to the baseline to ensure that no constraints are violated. \n",
    "\n",
    "Use the code below to check how many files have been created by the data capture, and view the latest file in detail. Note, data capture relies on data being sent to the production endpoint. If you don't see any files yet, wait several minutes and try again."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bucket = sagemaker_session.default_bucket()\r\n",
    "data_capture_logs_uri = 's3://{}/{}/datacapture/{}'.format(bucket, model_name, prd_endpoint_name)\r\n",
    "\r\n",
    "capture_files = S3Downloader.list(data_capture_logs_uri)\r\n",
    "print('Found {} files'.format(len(capture_files)))\r\n",
    "\r\n",
    "if capture_files:\r\n",
    "    # Get the first line of the most recent file    \r\n",
    "    event = json.loads(S3Downloader.read_file(capture_files[-1]).split('\\n')[0])\r\n",
    "    print('\\nLast file:\\n{}'.format(json.dumps(event, indent=2)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### View monitoring schedule\n",
    "\n",
    "There are some useful functions for plotting and rendering distribution statistics or constraint violations provided in a `utils` file in the [SageMaker Examples GitHub](https://github.com/aws/amazon-sagemaker-examples/tree/master/sagemaker_model_monitor/visualization)⇗. Grab a copy of this code to use in this notebook. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!wget -O utils.py --quiet https://raw.githubusercontent.com/awslabs/amazon-sagemaker-examples/master/sagemaker_model_monitor/visualization/utils.py\r\n",
    "import utils as mu"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The [minimum scheduled run time](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-scheduling.html)⇗ for model monitor is one hour, which means you will need to wait at least an hour to see any results. Use the code below to check the schedule status and list the next run. If you are completing this notebook as part of a workshop, your host will have activities which you can complete while you wait. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sm = boto3.client('sagemaker')\r\n",
    "\r\n",
    "response = sm.describe_monitoring_schedule(MonitoringScheduleName=schedule_name)\r\n",
    "print('Schedule Status: {}'.format(response['MonitoringScheduleStatus']))\r\n",
    "\r\n",
    "now = datetime.now(tzlocal())\r\n",
    "next_hour = (now+timedelta(hours=1)).replace(minute=0)\r\n",
    "scheduled_diff = (next_hour-now).seconds//60\r\n",
    "print('Next schedule in {} minutes'.format(scheduled_diff))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "While you wait, you can take a look at the CloudFormation template which is used as a base for the CloudFormation template built by CodeDeploy to deploy the production application. \n",
    "\n",
    "Alterntively, you can jump ahead to [Trigger Retraining](#Trigger-Retraining) which will kick off another run of the code pipeline whilst you wait."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!cat ../assets/deploy-model-prd.yml"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A couple of minutes after the model monitoring schedule has run, you can use the code below to fetch the latest schedule status.  A completed schedule run may have found violations. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "processing_job_arn = None\r\n",
    "\r\n",
    "while processing_job_arn == None:\r\n",
    "    try:\r\n",
    "        response = sm.list_monitoring_executions(MonitoringScheduleName=schedule_name)\r\n",
    "    except ClientError as e:\r\n",
    "        print(e.response[\"Error\"][\"Message\"])\r\n",
    "    for mon in response['MonitoringExecutionSummaries']:\r\n",
    "        status = mon['MonitoringExecutionStatus']\r\n",
    "        now = datetime.now(tzlocal())\r\n",
    "        created_diff = (now-mon['CreationTime']).seconds//60\r\n",
    "        print('Schedule status: {}, Created: {} minutes ago'.format(status, created_diff))\r\n",
    "        if status in ['Completed', 'CompletedWithViolations']:\r\n",
    "            processing_job_arn = mon['ProcessingJobArn']\r\n",
    "            break\r\n",
    "        if status == 'InProgress':\r\n",
    "            break\r\n",
    "    else:\r\n",
    "        raise(Exception('Please wait.  No Schedules executing'))\r\n",
    "    time.sleep(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### View monitoring results\n",
    "\n",
    "Once the model monitoring schedule has had a chance to run at least once, you can take a look at the results. First, load the monitoring execution results from the latest scheduled run."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if processing_job_arn:\r\n",
    "    execution = MonitoringExecution.from_processing_arn(sagemaker_session=sagemaker.Session(),\r\n",
    "                                                        processing_job_arn=processing_job_arn)\r\n",
    "    exec_inputs = {inp['InputName']: inp for inp in execution.describe()['ProcessingInputs']}\r\n",
    "    exec_results_uri = execution.output.destination\r\n",
    "\r\n",
    "    print('Monitoring Execution results: {}'.format(exec_results_uri))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Eche un vistazo a los archivos que se han guardado en la ubicación de salida de S3. Si se encontraron infracciones, debería ver un archivo de infracciones de restricciones además del archivo de estadísticas y restricciones que vio antes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!aws s3 ls $exec_results_uri/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora, buscamos las estadísticas de seguimiento y las infracciones. Luego usamos el código de utils para visualizar los resultados en una tabla. Esta resaltará cualquier desviación del baseline encontrada por el monitor del modelo. La desviación puede ocurrir para features categóricos (para estilos de cadena inferidos) o para features numéricas (por ejemplo, monto total de la tarifa)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get the baseline and monitoring statistics & violations\r\n",
    "baseline_statistics = baseline_job.baseline_statistics().body_dict\r\n",
    "execution_statistics = execution.statistics().body_dict\r\n",
    "violations = execution.constraint_violations().body_dict['violations']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mu.show_violation_df(baseline_statistics=baseline_statistics, \r\n",
    "                     latest_statistics=execution_statistics, \r\n",
    "                     violations=violations)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trigger Retraining\r\n",
    "\r\n",
    "La instancia de CodePipeline es configurada con [CloudWatch Events](https://docs.aws.amazon.com/codepipeline/latest/userguide/create-cloudtrail-S3-source.html)⇗ para empezar el pipeline para reentrenar cuando la detección de deriva (drift detection) activa alarmas métricas específicas.\r\n",
    "\r\n",
    "Puede simular la desviación colocando un valor de métrica por encima del umbral de \"0,2\" directamente en CloudWatch. Esto activará la alarma e iniciará code pipeline.\r\n",
    "\r\n",
    "<div class=\"alert alert-block alert-info\">\r\n",
    "    Tip:Esta alarma está configurada solo para el último endpoint de producción, por lo que el reentrenamiento solo se producirá si compara las métricas con el último endpoint.\r\n",
    "</div>\r\n",
    "\r\n",
    "![Metric graph in CloudWatch](../docs/cloudwatch-alarm.png)\r\n",
    "\r\n",
    "Ejecutamos el siguiente código para activar la alarma métrica. La salida de la celda será un enlace a CloudWatch, donde puede ver la alarma (similar a la captura de pantalla anterior) y un enlace a CodePipeline que verá que se ejecuta nuevamente. Tenga en cuenta que pueden pasar un par de minutos hasta que todo se active."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datetime import datetime\r\n",
    "import random\r\n",
    "\r\n",
    "cloudwatch = boto3.client('cloudwatch')\r\n",
    "\r\n",
    "# Define the metric name and threshold\r\n",
    "metric_name = 'feature_baseline_drift_total_amount'\r\n",
    "metric_threshold = 0.2\r\n",
    "\r\n",
    "# Put a new metric to trigger an alaram\r\n",
    "def put_drift_metric(value):\r\n",
    "    print('Putting metric: {}'.format(value))\r\n",
    "    response = cloudwatch.put_metric_data(\r\n",
    "        Namespace='aws/sagemaker/Endpoints/data-metrics',\r\n",
    "        MetricData=[\r\n",
    "            {\r\n",
    "                'MetricName': metric_name,\r\n",
    "                'Dimensions': [\r\n",
    "                    {\r\n",
    "                        'Name': 'MonitoringSchedule',\r\n",
    "                        'Value': schedule_name\r\n",
    "                    },\r\n",
    "                    {\r\n",
    "                        'Name': 'Endpoint',\r\n",
    "                        'Value': prd_endpoint_name\r\n",
    "                    },\r\n",
    "                ],\r\n",
    "                'Timestamp': datetime.now(),\r\n",
    "                'Value': value,\r\n",
    "                'Unit': 'None'\r\n",
    "            },\r\n",
    "        ]\r\n",
    "    )\r\n",
    "    \r\n",
    "def get_drift_stats():\r\n",
    "    response = cloudwatch.get_metric_statistics(\r\n",
    "        Namespace='aws/sagemaker/Endpoints/data-metrics',\r\n",
    "        MetricName=metric_name,\r\n",
    "        Dimensions=[\r\n",
    "            {\r\n",
    "                'Name': 'MonitoringSchedule',\r\n",
    "                'Value': schedule_name\r\n",
    "            },\r\n",
    "            {\r\n",
    "                'Name': 'Endpoint',\r\n",
    "                'Value': prd_endpoint_name\r\n",
    "            },\r\n",
    "        ],\r\n",
    "        StartTime=datetime.now() - timedelta(minutes=2),\r\n",
    "        EndTime=datetime.now(),\r\n",
    "        Period=1,\r\n",
    "        Statistics=['Average'],\r\n",
    "        Unit='None'\r\n",
    "    )\r\n",
    "    if 'Datapoints' in response and len(response['Datapoints']) > 0:        \r\n",
    "        return response['Datapoints'][0]['Average']\r\n",
    "    return 0    \r\n",
    "\r\n",
    "print('Simluate drift on endpoint: {}'.format(prd_endpoint_name))\r\n",
    "\r\n",
    "while True:\r\n",
    "    put_drift_metric(round(random.uniform(metric_threshold, 1.0), 4))\r\n",
    "    drift_stats = get_drift_stats()\r\n",
    "    print('Average drift amount: {}'.format(get_drift_stats()))\r\n",
    "    if drift_stats > metric_threshold:\r\n",
    "        break\r\n",
    "    time.sleep(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Haga clic en el historial de ejecución de alarmas y CodePipeline con los enlaces a continuación."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Output a html link to the cloudwatch dashboard\r\n",
    "metric_alarm_name = 'mlops-{}-metric-gt-threshold'.format(model_name)\r\n",
    "HTML('''<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/cloudwatch/home?region={0}#alarmsV2:alarm/{1}\">CloudWatch Alarm</a> triggers\r\n",
    "     <a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/codesuite/codepipeline/pipelines/{2}/executions?region={0}\">Code Pipeline Execution</a>'''.format(region, metric_alarm_name, pipeline_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Una vez que el pipeline se esté ejecutando de nuevo, podemos volver a [Inspect Training Job](#Inspect-Training-Job)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Synthetic Monitoring\r\n",
    "\r\n",
    "[Amazon CloudWatch Synthetics](https://aws.amazon.com/blogs/aws/new-use-cloudwatch-synthetics-to-monitor-sites-api-endpoints-web-workflows-and-more/) permite monitorear sitios, API REST y otros servicios implementados en AWS. Puede configurar un canary para probar que su API REST está devolviendo un valor esperado en un intervalo regular. Esta es una excelente manera de validar que la implementación blue/green no está causando ningún tiempo de inactividad para sus usuarios finales.\r\n",
    "\r\n",
    "Usamos el código a continuación para configurar un canary para probar continuamente la implementación de producción. Este canario simplemente hace ping a la API REST para probar si está en vivo, usando el código de `notebook / canary.js`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from urllib.parse import urlparse\r\n",
    "from string import Template\r\n",
    "from io import BytesIO\r\n",
    "import zipfile\r\n",
    "\r\n",
    "# Format the canary_js with rest_api and payload\r\n",
    "rest_url = urlparse(rest_api)\r\n",
    "\r\n",
    "with open('canary.js') as f:\r\n",
    "    canary_js = Template(f.read()).substitute(hostname=rest_url.netloc, path=rest_url.path, \r\n",
    "                                              data=payload.decode('utf-8').strip())\r\n",
    "# Write the zip file\r\n",
    "zip_buffer = BytesIO()\r\n",
    "with zipfile.ZipFile(zip_buffer, 'w') as zf:\r\n",
    "    zip_path = 'nodejs/node_modules/apiCanaryBlueprint.js' # Set a valid path\r\n",
    "    zip_info = zipfile.ZipInfo(zip_path)\r\n",
    "    zip_info.external_attr = 0o0755 << 16 # Ensure the file is readable\r\n",
    "    zf.writestr(zip_info, canary_js)\r\n",
    "zip_buffer.seek(0)\r\n",
    "\r\n",
    "# Create the canary\r\n",
    "synth = boto3.client('synthetics')\r\n",
    "\r\n",
    "role = sagemaker.get_execution_role()\r\n",
    "s3_canary_uri = 's3://{}/{}'.format(artifact_bucket, model_name)\r\n",
    "canary_name = 'mlops-{}'.format(model_name)\r\n",
    "\r\n",
    "try:\r\n",
    "    response = synth.create_canary(\r\n",
    "        Name=canary_name,\r\n",
    "        Code={\r\n",
    "            'ZipFile': bytearray(zip_buffer.read()),\r\n",
    "            'Handler': 'apiCanaryBlueprint.handler'\r\n",
    "        },\r\n",
    "        ArtifactS3Location=s3_canary_uri,\r\n",
    "        ExecutionRoleArn=role,\r\n",
    "        Schedule={ \r\n",
    "            'Expression': 'rate(10 minutes)', \r\n",
    "            'DurationInSeconds': 0 },\r\n",
    "        RunConfig={\r\n",
    "            'TimeoutInSeconds': 60,\r\n",
    "            'MemoryInMB': 960\r\n",
    "        },\r\n",
    "        SuccessRetentionPeriodInDays=31,\r\n",
    "        FailureRetentionPeriodInDays=31,\r\n",
    "        RuntimeVersion='syn-nodejs-2.0',\r\n",
    "    )\r\n",
    "    print('Creating canary: {}'.format(canary_name))    \r\n",
    "except ClientError as e:\r\n",
    "    if e.response[\"Error\"][\"Code\"] == \"AccessDeniedException\":\r\n",
    "        print('Canary not supported.') # Not supported in event engine\r\n",
    "    else:\r\n",
    "        raise(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora creamos una alarma de CloudWatch que se activará si la tasa de éxito del canario cae por debajo del 90%."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cloudwatch = boto3.client('cloudwatch')\r\n",
    "\r\n",
    "canary_alarm_name = '{}-synth-lt-threshold'.format(canary_name)\r\n",
    "\r\n",
    "response = cloudwatch.put_metric_alarm(\r\n",
    "    AlarmName=canary_alarm_name,\r\n",
    "    ComparisonOperator='LessThanThreshold',\r\n",
    "    EvaluationPeriods=1,\r\n",
    "    DatapointsToAlarm=1,\r\n",
    "    Period=600, # 10 minute interval\r\n",
    "    Statistic='Average',\r\n",
    "    Threshold=90.0,\r\n",
    "    ActionsEnabled=False,\r\n",
    "    AlarmDescription='SuccessPercent LessThanThreshold 90%',\r\n",
    "    Namespace='CloudWatchSynthetics',\r\n",
    "    MetricName='SuccessPercent',\r\n",
    "    Dimensions=[\r\n",
    "        {\r\n",
    "          'Name': 'CanaryName',\r\n",
    "          'Value': canary_name\r\n",
    "        },\r\n",
    "    ],\r\n",
    "    Unit='Seconds'\r\n",
    ")\r\n",
    "\r\n",
    "print('Creating alarm: {}'.format(canary_alarm_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ejecute el código a continuación para verificar si el canary se está ejecutando correctamente. La celda generará un enlace a la interfaz de usuario de CloudWatch Canaries, donde puede ver los resultados a lo largo del tiempo (ver captura de pantalla). El canary puede tardar un par de minutos en desplegarse.\r\n",
    "\r\n",
    "![Canary graph in CloudWatch](../docs/canary-green-1hr.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "while True:\r\n",
    "    try:\r\n",
    "        response = synth.get_canary(Name=canary_name)\r\n",
    "        status = response['Canary']['Status']['State']    \r\n",
    "        print('Canary status: {}'.format(status))\r\n",
    "        if status == 'ERROR':\r\n",
    "            raise(Exception(response['Canary']['Status']['StateReason']))    \r\n",
    "        elif status == 'READY':\r\n",
    "            synth.start_canary(Name=canary_name)\r\n",
    "        elif status == 'RUNNING':\r\n",
    "            break        \r\n",
    "    except ClientError as e:\r\n",
    "        if e.response[\"Error\"][\"Code\"] == \"ResourceNotFoundException\":\r\n",
    "            print('No canary found.')\r\n",
    "            break\r\n",
    "        elif e.response[\"Error\"][\"Code\"] == \"AccessDeniedException\":\r\n",
    "            print('Canary not supported.') # Not supported in event engine\r\n",
    "            break\r\n",
    "        print(e.response[\"Error\"][\"Message\"])\r\n",
    "    time.sleep(10)\r\n",
    "\r\n",
    "# Output a html link to the cloudwatch console\r\n",
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/cloudwatch/home?region={0}#synthetics:canary/detail/{1}\">CloudWatch Canary</a>'.format(region, canary_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Crear un dashboard en CloudWatch\r\n",
    "\r\n",
    "Por último, utilizaremos el código a continuación para crear un panel de CloudWatch para visualizar las alarmas y métricas de rendimiento clave que ha creado durante esta demo. La celda generará un enlace al dashboard. Este panel muestra 9 gráficos en tres filas, donde la primera fila muestra las métricas de Lambda, la segunda fila muestra las métricas de SageMaker y la tercera fila (que se muestra en la captura de pantalla a continuación) muestra las alarmas configuradas para el pipeline.\r\n",
    "\r\n",
    "![Graphs in CloudWatch dashboard](../docs/cloudwatch-dashboard.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sts = boto3.client('sts')\r\n",
    "account_id = sts.get_caller_identity().get('Account')\r\n",
    "dashboard_name = 'mlops-{}'.format(model_name)\r\n",
    "\r\n",
    "with open('dashboard.json') as f:\r\n",
    "    dashboard_body = Template(f.read()).substitute(region=region, account_id=account_id, model_name=model_name)\r\n",
    "    response = cloudwatch.put_dashboard(\r\n",
    "        DashboardName=dashboard_name,\r\n",
    "        DashboardBody=dashboard_body\r\n",
    "    )\r\n",
    "\r\n",
    "# Output a html link to the cloudwatch dashboard\r\n",
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/cloudwatch/home?region={0}#dashboards:name={1}\">CloudWatch Dashboard</a>'.format(region, canary_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Congratulations! You have made it to the end of this notebook, and have automated a safe MLOps pipeline using a wide range of AWS services. \n",
    "\n",
    "You can use the other notebook in this repository [workflow.ipynb](workflow.ipynb) to implement your own ML model and deploy it as part of this pipeline. Or, if you are finished with the content, follow the instructions in the next section to clean up the resources you have deployed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cleanup\n",
    "\n",
    "Execute the following cell to delete the stacks created in the pipeline. For a model name of **nyctaxi** these would be:\n",
    "\n",
    "1. *nyctaxi*-deploy-prd\n",
    "2. *nyctaxi*-deploy-dev\n",
    "3. *nyctaxi*-workflow\n",
    "4. sagemaker-custom-resource"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cfn = boto3.client('cloudformation')\r\n",
    "\r\n",
    "# Delete the prod and then dev stack\r\n",
    "for stack_name in [f'{pipeline_name}-deploy-prd', \r\n",
    "                   f'{pipeline_name}-deploy-dev',\r\n",
    "                   f'{pipeline_name}-workflow',\r\n",
    "                   'sagemaker-custom-resource']:\r\n",
    "    print('Deleting stack: {}'.format(stack_name))\r\n",
    "    cfn.delete_stack(StackName=stack_name)\r\n",
    "    cfn.get_waiter('stack_delete_complete').wait(StackName=stack_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code will stop and delete the canary you created."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "while True:\r\n",
    "    try:\r\n",
    "        response = synth.get_canary(Name=canary_name)\r\n",
    "        status = response['Canary']['Status']['State']    \r\n",
    "        print('Canary status: {}'.format(status))\r\n",
    "        if status == 'ERROR':\r\n",
    "            raise(Exception(response['Canary']['Status']['StateReason']))    \r\n",
    "        elif status == 'STOPPED':\r\n",
    "            synth.delete_canary(Name=canary_name)\r\n",
    "        elif status == 'RUNNING':\r\n",
    "            synth.stop_canary(Name=canary_name)\r\n",
    "    except ClientError as e:\r\n",
    "        if e.response[\"Error\"][\"Code\"] == \"ResourceNotFoundException\":\r\n",
    "            print('Canary succesfully deleted.')\r\n",
    "            break\r\n",
    "        elif e.response[\"Error\"][\"Code\"] == \"AccessDeniedException\":\r\n",
    "            print('Canary not created.') # Not supported in event engine\r\n",
    "            break\r\n",
    "        print(e.response[\"Error\"][\"Message\"])\r\n",
    "    time.sleep(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code will delete the dashboard."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cloudwatch.delete_alarms(AlarmNames=[canary_alarm_name])\r\n",
    "print('Alarm deleted')\r\n",
    "\r\n",
    "cloudwatch.delete_dashboards(DashboardNames=[dashboard_name])\r\n",
    "print('Dashboard deleted')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, close this notebook and you can delete the CloudFormation you created to launch this MLOps sample."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}