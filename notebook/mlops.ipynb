{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MLOps Deployment Pipeline\r\n",
    "\r\n",
    "\r\n",
    "## Overview\r\n",
    "\r\n",
    "En este notebook, iremos paso a paso por un pipeline de MLOps para construir, entrenar, implementar y monitorear un modelo de regresión XGBoost que predice la tarifa de taxi esperada usando el [dataset](https://registry.opendata.aws/nyc-tlc-trip-records-pds/) \"New York City Taxi\". Este pipeline presenta una estrategia de [implementación canaria](https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/canary-deployment.html) con reversión en caso de error. La idea es poder entender cómo activar y monitorear el pipeline, inspeccionar el flujo de trabajo de entrenamiento, usar model monitor para configurar alertas y crear una implementación canary.\r\n",
    "\r\n",
    "### Contenido\r\n",
    "\r\n",
    "Este notebook contiene las siguientes secciones:\r\n",
    "\r\n",
    "1. [Data Prep](#Data-Prep)\r\n",
    "2. [Build](#Build)\r\n",
    "3. [Train Model](#Train-Model)\r\n",
    "4. [Deploy Dev](#Deploy-Dev)\r\n",
    "5. [Deploy Prod](#Deploy-Prod)\r\n",
    "6. [Monitor](#Monitor)\r\n",
    "6. [Cleanup](#Cleanup)\r\n",
    "\r\n",
    "### Arquitectura\r\n",
    "\r\n",
    "El diagrama de arquitectura a continuación muestra todo el pipeline de MLOps a un alto nivel.\r\n",
    "\r\n",
    "Usaremos la plantilla de CloudFormation proporcionada en este repositorio (`pipeline.yml`) para crear una demo en su propia cuenta de AWS. CloudFormation implementa varios recursos:\r\n",
    "   \r\n",
    "1. A customer-managed encryption key in in Amazon KMS for encrypting data and artifacts.\r\n",
    "1. A secret in Amazon Secrets Manager to securely store your GitHub Access Token.\r\n",
    "1. Several AWS IAM roles so CloudFormation, SageMaker, and other AWS services can perform actions in your AWS account, following the principle of [least privilege](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege)⇗.\r\n",
    "1. A messaging service in Amazon SNS to notify you when CodeDeploy has successfully deployed the API, and to receive alerts for retraining and drift detection (signing up for these notifications is optional).\r\n",
    "1. Two Amazon CloudWatch event rules: one which schedules the pipeline to run every month, and one which triggers the pipeline to run when SageMaker Model Monitor detects certain metrics.\r\n",
    "1. An Amazon SageMaker Jupyter notebook with this workshop content pre-loaded.\r\n",
    "1. An Amazon S3 bucket for storing model artifacts.\r\n",
    "1. An AWS CodePipeline instance with several pre-defined stages. \r\n",
    "\r\n",
    "Take a moment to look at all of these resources now deployed in your account. \r\n",
    "\r\n",
    "![MLOps pipeline architecture](../docs/mlops-architecture.png)\r\n",
    "\r\n",
    "In this notebook, you will work through the CodePipeline instance created by the CloudFormation template. It has several stages:\r\n",
    "\r\n",
    "1. **Source** - The pipeline is already configured with two sources. If you upload a new dataset to a specific location in the S3 data bucket, this will trigger the pipeline to run. The Git source can be GitHub, or CodeCommit if you don’t supply your access token. If you commit new code to your repository, this will trigger the pipeline to run. \r\n",
    "1. **Build** - In this stage, CodeBuild configured by the build specification `model/buildspec.yml` will execute `model/run.py` to generate AWS CloudFormation templates for creating the AWS Step Function (including AWS Lambda custom resources), and deployment templates used in the following stages based on the data sets and hyperparameters specified for this pipeline run. You will take a closer look at these files later in this notebook. \r\n",
    "1. **Train** The Step Functions workflow created in the Build stage is run in this stage. The workflow creates a baseline for the model monitor using a SageMaker processing job, and trains an XGBoost model on the taxi ride dataset using a SageMaker training job.\r\n",
    "1. **Deploy Dev** In this stage, a CloudFormation template created in the build stage (from `assets/deploy-model-dev.yml`) deploys a dev endpoint. This will allow you to run tests on the model and decide if the model is of sufficient quality to deploy into production.\r\n",
    "1. **Deploy Production** The final stage of the pipeline is the only stage which does not run automatically as soon as the previous stage is complete. It waits for a user to manually approve the model which was previously deployed to dev. As soon as the model is approved, a CloudFormation template (packaged from `assets/deploy-model-prod.yml` to include the Lambda functions saved and uploaded as ZIP files in S3) deploys the production endpoint. It configures autoscaling and enables data capture. It creates a model monitoring schedule and sets CloudWatch alarms for certain metrics. It also sets up an AWS CodeDeploy instance which deploys a set of AWS Lambda functions and an Amazon API Gateway to sit in front of the SageMaker endpoint. This stage can make use of canary deployment to safely switch from an old model to a new model. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import the latest sagemaker and boto3 SDKs.\r\n",
    "import sys\r\n",
    "!{sys.executable} -m pip install --upgrade pip\r\n",
    "!{sys.executable} -m pip install -qU awscli boto3 \"sagemaker>=2.1.0<3\" tqdm\r\n",
    "!{sys.executable} -m pip install -qU \"stepfunctions==2.0.0\"\r\n",
    "!{sys.executable} -m pip show sagemaker stepfunctions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Restart your SageMaker kernel then continue with this notebook."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Prep\n",
    " \n",
    "In this section of the notebook, you will download the publicly available New York Taxi dataset in preparation for uploading it to S3.\n",
    "\n",
    "### Download Dataset\n",
    "\n",
    "First, download a sample of the New York City Taxi [dataset](https://registry.opendata.aws/nyc-tlc-trip-records-pds/)⇗ to this notebook instance. This dataset contains information on trips taken by taxis and for-hire vehicles in New York City, including pick-up and drop-off times and locations, fares, distance traveled, and more. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!aws s3 cp 's3://nyc-tlc/trip data/green_tripdata_2018-02.csv' 'nyc-tlc.csv'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now load the dataset into a pandas data frame, taking care to parse the dates correctly."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "parse_dates= ['lpep_dropoff_datetime', 'lpep_pickup_datetime']\r\n",
    "trip_df = pd.read_csv('nyc-tlc.csv', parse_dates=parse_dates)\r\n",
    "\r\n",
    "trip_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data manipulation\n",
    "\n",
    "Instead of the raw date and time features for pick-up and drop-off, let's use these features to calculate the total time of the trip in minutes, which will be easier to work with for our model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trip_df['duration_minutes'] = (trip_df['lpep_dropoff_datetime'] - trip_df['lpep_pickup_datetime']).dt.seconds/60"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset contains a lot of columns we don't need, so let's select a sample of columns for our machine learning model. Keep only `total_amount` (fare), `duration_minutes`, `passenger_count`, and `trip_distance`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cols = ['total_amount', 'duration_minutes', 'passenger_count', 'trip_distance']\r\n",
    "data_df = trip_df[cols]\r\n",
    "print(data_df.shape)\r\n",
    "data_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate some quick statistics for the dataset to understand the quality."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_df.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The table above shows some clear outliers, e.g. -400 or 2626 as fare, or 0 passengers. There are many intelligent methods for identifying and removing outliers, but data cleaning is not the focus of this notebook, so just remove the outliers by setting some min and max values which seem more reasonable. Removing the outliers results in a final dataset of 754,671 rows."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_df = data_df[(data_df.total_amount > 0) & (data_df.total_amount < 200) & \r\n",
    "                  (data_df.duration_minutes > 0) & (data_df.duration_minutes < 120) & \r\n",
    "                  (data_df.trip_distance > 0) & (data_df.trip_distance < 121) & \r\n",
    "                  (data_df.passenger_count > 0)].dropna()\r\n",
    "print(data_df.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data visualization\n",
    "\n",
    "Since this notebook will build a regression model for the taxi data, it's a good idea to check if there is any correlation between the variables in our data. Use scatter plots on a sample of the data to compare trip distance with duration in minutes, and total amount (fare) with duration in minutes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import seaborn as sns \r\n",
    "\r\n",
    "sample_df = data_df.sample(1000)\r\n",
    "sns.scatterplot(data=sample_df, x='duration_minutes', y='trip_distance')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.scatterplot(data=sample_df, x='duration_minutes', y='total_amount')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "These scatter plots look fine and show at least some correlation between our variables. \n",
    "\n",
    "### Data splitting and saving\n",
    "\n",
    "We are now ready to split the dataset into train, validation, and test sets. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "train_df, val_df = train_test_split(data_df, test_size=0.20, random_state=42)\r\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.05, random_state=42)\r\n",
    "\r\n",
    "# Reset the index for our test dataframe\r\n",
    "test_df.reset_index(inplace=True, drop=True)\r\n",
    "\r\n",
    "print('Size of\\n train: {},\\n val: {},\\n test: {} '.format(train_df.shape[0], val_df.shape[0], test_df.shape[0]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the train, validation, and test files as CSV locally on this notebook instance. Notice that you save the train file twice - once as the training data file and once as the baseline data file. The baseline data file will be used by [SageMaker Model Monitor](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html)⇗ to detect data drift. Data drift occurs when the statistical nature of the data that your model receives while in production drifts away from the nature of the baseline data it was trained on, which means the model begins to lose accuracy in its predictions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_cols = ['total_amount', 'duration_minutes','passenger_count','trip_distance']\r\n",
    "train_df.to_csv('train.csv', index=False, header=False)\r\n",
    "val_df.to_csv('validation.csv', index=False, header=False)\r\n",
    "test_df.to_csv('test.csv', index=False, header=False)\r\n",
    "\r\n",
    "# Save test and baseline with headers\r\n",
    "train_df.to_csv('baseline.csv', index=False, header=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now upload these CSV files to your default SageMaker S3 bucket. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sagemaker\r\n",
    "\r\n",
    "# Get the session and default bucket\r\n",
    "session = sagemaker.session.Session()\r\n",
    "bucket = session.default_bucket()\r\n",
    "\r\n",
    "# Specify data prefix and version\r\n",
    "prefix = 'nyc-tlc/v1'\r\n",
    "\r\n",
    "s3_train_uri = session.upload_data('train.csv', bucket, prefix + '/data/training')\r\n",
    "s3_val_uri = session.upload_data('validation.csv', bucket, prefix + '/data/validation')\r\n",
    "s3_test_uri = session.upload_data('test.csv', bucket, prefix + '/data/test')\r\n",
    "s3_baseline_uri = session.upload_data('baseline.csv', bucket, prefix + '/data/baseline')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You will use the datasets which you have prepared and saved in this section to trigger the pipeline to train and deploy a model in the next section."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build\n",
    "\n",
    "If you navigate to the CodePipeline instance created for this workshop, you will notice that the Source stage is initially in a `Failed` state. This happens because the dataset, which is one of the sources that can trigger the pipeline, has not yet been uploaded to the S3 location expected by the pipeline.\n",
    "\n",
    "![Failed code pipeline](../docs/pipeline_failed.png)\n",
    "\n",
    "### Trigger Build\n",
    "\n",
    "In this section, you will start a model build and deployment pipeline by packaging up the datasets you prepared in the previous section and uploading these to the S3 source location which triggers the CodePipeline instance created for this workshop. \n",
    "\n",
    "First, import some libraries and load some environment variables which you will need. These environment variables have been set through a  [lifecycle configuration](https://docs.aws.amazon.com/sagemaker/latest/dg/notebook-lifecycle-config.html)⇗ script attached to this notebook."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import boto3\r\n",
    "from botocore.exceptions import ClientError\r\n",
    "import os\r\n",
    "import time\r\n",
    "\r\n",
    "region = boto3.Session().region_name\r\n",
    "artifact_bucket = os.environ['ARTIFACT_BUCKET']\r\n",
    "pipeline_name = os.environ['PIPELINE_NAME']\r\n",
    "model_name = os.environ['MODEL_NAME']\r\n",
    "workflow_pipeline_arn = os.environ['WORKFLOW_PIPELINE_ARN']\r\n",
    "\r\n",
    "print('region: {}'.format(region))\r\n",
    "print('artifact bucket: {}'.format(artifact_bucket))\r\n",
    "print('pipeline: {}'.format(pipeline_name))\r\n",
    "print('model name: {}'.format(model_name))\r\n",
    "print('workflow: {}'.format(workflow_pipeline_arn))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the AWS CodePipeline [documentation](https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-simple-s3.html)⇗:\n",
    "\n",
    "> When Amazon S3 is the source provider for your pipeline, you may zip your source file or files into a single .zip and upload the .zip to your source bucket. You may also upload a single unzipped file; however, downstream actions that expect a .zip file will fail.\n",
    "\n",
    "To train a model, you need multiple datasets (train, validation, and test) along with a file specifying the hyperparameters. In this example, you will create one JSON file which contains the S3 dataset locations and one JSON file which contains the hyperparameter values. Then you compress both files into a zip package to be used as input for the pipeline run. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from io import BytesIO\r\n",
    "import zipfile\r\n",
    "import json\r\n",
    "\r\n",
    "input_data = {\r\n",
    "    'TrainingUri': s3_train_uri,\r\n",
    "    'ValidationUri': s3_val_uri,\r\n",
    "    'TestUri': s3_test_uri,\r\n",
    "    'BaselineUri': s3_baseline_uri\r\n",
    "}\r\n",
    "\r\n",
    "hyperparameters = {\r\n",
    "    'num_round': 50\r\n",
    "}\r\n",
    "\r\n",
    "zip_buffer = BytesIO()\r\n",
    "with zipfile.ZipFile(zip_buffer, 'a') as zf:\r\n",
    "    zf.writestr('inputData.json', json.dumps(input_data))\r\n",
    "    zf.writestr('hyperparameters.json', json.dumps(hyperparameters))\r\n",
    "zip_buffer.seek(0)\r\n",
    "\r\n",
    "data_source_key = '{}/data-source.zip'.format(pipeline_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now upload the zip package to your artifact S3 bucket - this action will trigger the pipeline to train and deploy a model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "s3 = boto3.client('s3')\r\n",
    "s3.put_object(Bucket=artifact_bucket, Key=data_source_key, Body=bytearray(zip_buffer.read()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Click the link below to open the AWS console at the Code Pipeline if you don't have it open in another tab.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    Tip: You may need to wait a minute to see the DataSource stage turn green. The page will refresh automatically.\n",
    "</div>\n",
    "\n",
    "![Source Green](../docs/datasource-after.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from IPython.core.display import HTML\r\n",
    "\r\n",
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/codesuite/codepipeline/pipelines/{1}/view?region={0}\">Code Pipeline</a>'.format(region, pipeline_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspect Build Logs\n",
    "\n",
    "Once the build stage is running, you will see the AWS CodeBuild job turn blue with a status of **In progress**.\n",
    "\n",
    "![Failed code pipeline](../docs/codebuild-inprogress.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can click on the **Details** link displayed in the CodePipeline UI or click the link below to jump directly to the CodeBuild logs.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    Tip: You may need to wait a few seconds for the pipeline to transition into the active (blue) state and for the build to start.\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "codepipeline = boto3.client('codepipeline')\r\n",
    "\r\n",
    "def get_pipeline_stage(pipeline_name, stage_name):\r\n",
    "    response = codepipeline.get_pipeline_state(name=pipeline_name)\r\n",
    "    for stage in response['stageStates']:\r\n",
    "        if stage['stageName'] == stage_name:\r\n",
    "            return stage\r\n",
    "\r\n",
    "# Get last execution id\r\n",
    "build_stage = get_pipeline_stage(pipeline_name, 'Build')    \r\n",
    "if not 'latestExecution' in build_stage:\r\n",
    "    raise(Exception('Please wait.  Build not started'))\r\n",
    "\r\n",
    "build_url = build_stage['actionStates'][0]['latestExecution']['externalExecutionUrl']\r\n",
    "\r\n",
    "# Out a link to the code build logs\r\n",
    "HTML('<a target=\"_blank\" href=\"{0}\">Code Build Logs</a>'.format(build_url))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The AWS CodeBuild process is responsible for creating a number of AWS CloudFormation templates which we will explore in more detail in the next section.  Two of these templates are used to set up the **Train** step by creating the AWS Step Functions worklow and the custom AWS Lambda functions used within this workflow."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Model\n",
    "\n",
    "### Inspect Training Job\n",
    "\n",
    "Wait until the pipeline has started running the Train step (see screenshot) before continuing with the next cells in this notebook. \n",
    "\n",
    "![Training in progress](../docs/train-in-progress.png)\n",
    "\n",
    "When the pipeline has started running the train step, you can click on the **Details** link displayed in the CodePipeline UI (see screenshot above) to view the Step Functions workflow which is running the training job. \n",
    "\n",
    "Alternatively, you can click on the Workflow link from the cell output below once it's available."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from stepfunctions.workflow import Workflow\r\n",
    "while True:\r\n",
    "    try:\r\n",
    "        workflow = Workflow.attach(workflow_pipeline_arn)\r\n",
    "        break\r\n",
    "    except ClientError as e:\r\n",
    "        print(e.response[\"Error\"][\"Message\"])\r\n",
    "    time.sleep(10)\r\n",
    "\r\n",
    "workflow"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or simply run the cell below to display the Step Functions workflow, and re-run it after a few minutes to see the progress."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "executions = workflow.list_executions()\r\n",
    "if not executions:\r\n",
    "    raise(Exception('Please wait.  Training not started'))\r\n",
    "    \r\n",
    "executions[0].render_progress()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Review Build Script\n",
    "\n",
    "While you wait for the training job to complete, let's take a look at the `run.py` code which was used by the AWS CodeBuild process.\n",
    "\n",
    "This script takes all of the input parameters, including the dataset locations and hyperparameters which you saved to JSON files earlier in this notebook, and uses them to generate the templates which the pipeline needs to run the training job. It *does not* create the actual Step Functions instance - it only generates the templates which define the Step Functions workflow, as well as the CloudFormation input templates which CodePipeline uses to instantiate the Step Functions instance.\n",
    "\n",
    "Step-by-step, the script does the following:\n",
    "\n",
    "1. It collects all the input parameters it needs to generate the templates. This includes information about the environment container needed to run the training job, the input and output data locations, IAM roles needed by various components, encryption keys, and more. It then sets up some basic parameters like the AWS region and the function names.\n",
    "1. If the input parameters specify an environment container stored in ECR, it fetches that container. Otherwise, it fetches the URI of the AWS managed environment container needed for the training job.\n",
    "1. It reads the input data JSON file which you generated earlier in this notebook (and which was included in the zip source for the pipeline), thereby fetching the locations of the train, validation, and baseline data files. Then it formats more parameters which will be needed later in the script, including version IDs and output data locations.\n",
    "1. It reads the hyperparameter JSON file which you generated earlier in this notebook.\n",
    "1. It defines the Step Functions workflow, starting with the input schema, followed by each step of the workflow (i.e. Create Experiment, Baseline Job, Training Job), and finally combines those steps into a workflow graph. \n",
    "1. The workflow graph is saved to file, along with a file containing all of the input parameters saved according to the schema defined in the workflow.\n",
    "1. It saves parameters to file which will be used by CloudFormation to instantiate the Step Functions workflow."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pygmentize ../model/run.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Customize Workflow (Optional)\n",
    "\n",
    "If you are interested in customising the workflow used in the Build Script, store the `input_data` to be used within the local [workflow.ipynb](workflow.ipynb) notebook. The workflow notebook can be used to experiment with the Step Functions workflow and training job definitions for your model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%store input_data"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Analytics\n",
    "\n",
    "Once the training and baseline jobs are complete (meaning they are displayed in a green color in the Step Functions workflow, this takes around 5 minutes), you can inspect the experiment metrics. The code below will display all experiments in a table. Note that the baseline processing job won't have RMSE metrics - it calculates metrics based on the training data, but does not train a machine learning model. \n",
    "\n",
    "You will [explore the baseline](#Explore-Baseline) results later in this notebook. <a id=\"validation-results\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sagemaker import analytics\r\n",
    "experiment_name = 'mlops-{}'.format(model_name)\r\n",
    "model_analytics = analytics.ExperimentAnalytics(experiment_name=experiment_name)\r\n",
    "analytics_df = model_analytics.dataframe()\r\n",
    "\r\n",
    "if (analytics_df.shape[0] == 0):\r\n",
    "    raise(Exception('Please wait.  No training or baseline jobs'))\r\n",
    "\r\n",
    "pd.set_option('display.max_colwidth', 100) # Increase column width to show full copmontent name\r\n",
    "cols = ['TrialComponentName', 'DisplayName', 'SageMaker.InstanceType', \r\n",
    "        'train:rmse - Last', 'validation:rmse - Last'] # return the last rmse for training and validation\r\n",
    "analytics_df[analytics_df.columns & cols].head(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deploy Dev\n",
    "\n",
    "### Test Dev Deployment\n",
    "\n",
    "When the pipeline has finished training a model, it automatically moves to the next step, where the model is deployed as a SageMaker Endpoint. This endpoint is part of your dev deployment, therefore, in this section, you will run some tests on the endpoint to decide if you want to deploy this model into production.\n",
    "\n",
    "First, run the cell below to fetch the name of the SageMaker Endpoint."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "codepipeline = boto3.client('codepipeline')\r\n",
    "\r\n",
    "deploy_dev = get_pipeline_stage(pipeline_name, 'DeployDev')\r\n",
    "if not 'latestExecution' in deploy_dev:\r\n",
    "    raise(Exception('Please wait.  Deploy dev not started'))\r\n",
    "    \r\n",
    "execution_id = deploy_dev['latestExecution']['pipelineExecutionId']\r\n",
    "dev_endpoint_name = 'mlops-{}-dev-{}'.format(model_name, execution_id)\r\n",
    "\r\n",
    "print('endpoint name: {}'.format(dev_endpoint_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you moved through the previous section very quickly, you will need to wait until the dev endpoint has been successfully deployed and the pipeline is waiting for approval to deploy to production (see screenshot). It can take up to 10 minutes for SageMaker to create an endpoint.\n",
    "\n",
    "![Deploying dev endpoint in code pipeline](../docs/dev-deploy-ready.png)\n",
    "\n",
    "Alternatively, run the code below to check the status of your endpoint. Wait until the status of the endpoint is 'InService'."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sm = boto3.client('sagemaker')\r\n",
    "\r\n",
    "while True:\r\n",
    "    try:\r\n",
    "        response = sm.describe_endpoint(EndpointName=dev_endpoint_name)\r\n",
    "        print(\"Endpoint status: {}\".format(response['EndpointStatus']))\r\n",
    "        if response['EndpointStatus'] == 'InService':\r\n",
    "            break\r\n",
    "    except ClientError as e:\r\n",
    "        print(e.response[\"Error\"][\"Message\"])\r\n",
    "    time.sleep(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that your endpoint is ready, let's write some code to run the test data (which you split off from the dataset and saved to file at the start of this notebook) through the endpoint for inference. The code below supports both v1 and v2 of the SageMaker SDK, but we recommend using v2 of the SDK in all of your future projects."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "try:\r\n",
    "    # Support SageMaker v2 SDK: https://sagemaker.readthedocs.io/en/stable/v2.html\r\n",
    "    from sagemaker.predictor import Predictor\r\n",
    "    from sagemaker.serializers import CSVSerializer\r\n",
    "    def get_predictor(endpoint_name):\r\n",
    "        xgb_predictor = Predictor(endpoint_name)\r\n",
    "        xgb_predictor.serializer = CSVSerializer()\r\n",
    "        return xgb_predictor\r\n",
    "except:\r\n",
    "    # Fallback to SageMaker v1.70 SDK\r\n",
    "    from sagemaker.predictor import RealTimePredictor, csv_serializer\r\n",
    "    def get_predictor(endpoint_name):\r\n",
    "        xgb_predictor = RealTimePredictor(endpoint_name)\r\n",
    "        xgb_predictor.content_type = 'text/csv'\r\n",
    "        xgb_predictor.serializer = csv_serializer\r\n",
    "        return xgb_predictor\r\n",
    "\r\n",
    "def predict(predictor, data, rows=500):\r\n",
    "    split_array = np.array_split(data, round(data.shape[0] / float(rows)))\r\n",
    "    predictions = ''\r\n",
    "    for array in tqdm(split_array):\r\n",
    "        predictions = ','.join([predictions, predictor.predict(array).decode('utf-8')])\r\n",
    "    return np.fromstring(predictions[1:], sep=',')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now use the `predict` function, which was defined in the code above, to run the test data through the endpoint and generate the predictions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dev_predictor = get_predictor(dev_endpoint_name)\r\n",
    "predictions = predict(dev_predictor, test_df[test_df.columns[1:]].values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, load the predictions into a data frame, and join it with your test data. Then, calculate absolute error as the difference between the actual taxi fare and the predicted taxi fare. Display the results in a table, sorted by the highest absolute error values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred_df = pd.DataFrame({'total_amount_predictions': predictions })\r\n",
    "pred_df = test_df.join(pred_df) # Join on all\r\n",
    "pred_df['error'] = abs(pred_df['total_amount']-pred_df['total_amount_predictions'])\r\n",
    "\r\n",
    "pred_df.sort_values('error', ascending=False).head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From this table, we note that some short trip distances have large errors because the low predicted fare does not match the high actual fare. This could be the result of a generous tip which we haven't included in this dataset.\n",
    "\n",
    "You can also analyze the results by plotting the absolute error to visualize outliers. In this graph, we see that most of the outliers are cases where the model predicted a much lower fare than the actual fare. There are only a few outliers where the model predicted a higher fare than the actual fare."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.scatterplot(data=pred_df, x='total_amount_predictions', y='total_amount', hue='error')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you want one overall measure of quality for the model, you can calculate the root mean square error (RMSE) for the predicted fares compared to the actual fares. Compare this to the [results calculated on the validation set](#validation-results) at the end of the 'Inspect Training Job' section."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from math import sqrt\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "\r\n",
    "def rmse(pred_df):\r\n",
    "    return sqrt(mean_squared_error(pred_df['total_amount'], pred_df['total_amount_predictions']))\r\n",
    "\r\n",
    "print('RMSE: {}'.format(rmse(pred_df)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deploy Prod\n",
    "\n",
    "### Approve Deployment to Production\n",
    "\n",
    "If you are happy with the results of the model, you can go ahead and approve the model to be deployed into production. You can do so by clicking the **Review** button in the CodePipeline UI, leaving a comment to explain why you approve this model, and clicking on **Approve**. \n",
    "\n",
    "Alternatively, you can create a Jupyter widget which (when enabled) allows you to comment and approve the model directly from this notebook. Run the cell below to see this in action."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import ipywidgets as widgets\r\n",
    "\r\n",
    "def on_click(obj):\r\n",
    "    result = { 'summary': approval_text.value, 'status': obj.description }\r\n",
    "    response = codepipeline.put_approval_result(\r\n",
    "      pipelineName=pipeline_name,\r\n",
    "      stageName='DeployDev',\r\n",
    "      actionName='ApproveDeploy',\r\n",
    "      result=result,\r\n",
    "      token=approval_action['token']\r\n",
    "    )\r\n",
    "    button_box.close()\r\n",
    "    print(result)\r\n",
    "    \r\n",
    "# Create the widget if we are ready for approval\r\n",
    "deploy_dev = get_pipeline_stage(pipeline_name, 'DeployDev')\r\n",
    "if not 'latestExecution' in deploy_dev['actionStates'][-1]:\r\n",
    "    raise(Exception('Please wait.  Deploy dev not complete'))\r\n",
    "\r\n",
    "approval_action = deploy_dev['actionStates'][-1]['latestExecution']\r\n",
    "if approval_action['status'] == 'Succeeded':\r\n",
    "    print('Dev approved: {}'.format(approval_action['summary']))\r\n",
    "elif 'token' in approval_action:\r\n",
    "    approval_text = widgets.Text(placeholder='Optional approval message')   \r\n",
    "    approve_btn = widgets.Button(description=\"Approved\", button_style='success', icon='check')\r\n",
    "    reject_btn = widgets.Button(description=\"Rejected\", button_style='danger', icon='close')\r\n",
    "    approve_btn.on_click(on_click)\r\n",
    "    reject_btn.on_click(on_click)\r\n",
    "    button_box = widgets.HBox([approval_text, approve_btn, reject_btn])\r\n",
    "    display(button_box)\r\n",
    "else:\r\n",
    "    raise(Exception('Please wait. No dev approval'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Production Deployment\n",
    "\n",
    "Within about a minute after approving the model deployment, you should see the pipeline start on the final step: deploying your model into production. In this section, you will check the deployment status and test the production endpoint after it has been deployed.\n",
    "\n",
    "![Deploy production endpoint in code pipeline](../docs/deploy-production.png)\n",
    "\n",
    "This step of the pipeline uses CloudFormation to deploy a number of resources on your behalf. In particular, it creates:\n",
    "\n",
    "1. A production-ready SageMaker Endpoint for your model, with [data capture](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-capture.html)⇗  (used by SageMaker Model Monitor) and [autoscaling](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html)⇗ enabled.\n",
    "1. A [model monitoring schedule](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-scheduling.html)⇗ which outputs the results to CloudWatch metrics, along with a [CloudWatch Alarm](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html)⇗ which will notify you when a violation occurs. \n",
    "1. A CodeDeploy instance which creates a simple app by deploying API Gateway, three Lambda functions, and an alarm to notify of the success or failure of this deployment. The code for the Lambda functions can be found in `api/app.py`, `api/pre_traffic_hook.py`, and `api/post_traffic_hook.py`. These functions update the endpoint to enable data capture, format and submit incoming traffic to the SageMaker endpoint, and capture the data logs.\n",
    "\n",
    "![Components of production deployment](../docs/cloud-formation.png)\n",
    "\n",
    "Let's check how the deployment is progressing. Use the code below to fetch the execution ID of the depoyment step. Then generate a table which lists the resources created by the CloudFormation stack and their creation status. You can re-run the cell after a few minutes to see how the steps are progressing."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "deploy_prd = get_pipeline_stage(pipeline_name, 'DeployPrd')\r\n",
    "if not 'latestExecution' in deploy_prd or not 'latestExecution' in deploy_prd['actionStates'][0]:\r\n",
    "    raise(Exception('Please wait.  Deploy prd not started'))\r\n",
    "    \r\n",
    "execution_id = deploy_prd['latestExecution']['pipelineExecutionId']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datetime import datetime, timedelta\r\n",
    "from dateutil.tz import tzlocal\r\n",
    "\r\n",
    "def get_event_dataframe(events):\r\n",
    "    stack_cols = ['LogicalResourceId', 'ResourceStatus', 'ResourceStatusReason', 'Timestamp']\r\n",
    "    stack_event_df = pd.DataFrame(events)[stack_cols].fillna('')\r\n",
    "    stack_event_df['TimeAgo'] = (datetime.now(tzlocal())-stack_event_df['Timestamp'])\r\n",
    "    return stack_event_df.drop('Timestamp', axis=1)\r\n",
    "\r\n",
    "cfn = boto3.client('cloudformation')\r\n",
    "\r\n",
    "stack_name = stack_name='{}-deploy-prd'.format(pipeline_name)\r\n",
    "print('stack name: {}'.format(stack_name))\r\n",
    "\r\n",
    "# Get latest stack events\r\n",
    "while True:\r\n",
    "    try:\r\n",
    "        response = cfn.describe_stack_events(StackName=stack_name)\r\n",
    "        break\r\n",
    "    except ClientError as e:\r\n",
    "        print(e.response[\"Error\"][\"Message\"])\r\n",
    "    time.sleep(10)\r\n",
    "    \r\n",
    "get_event_dataframe(response['StackEvents']).head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The resource of most interest to us is the endpoint. This takes on average 10 minutes to deploy.  In the meantime, you can take a look at the Python code used for the application. \n",
    "\n",
    "The `app.py` is the main entry point invoking the Amazon SageMaker endpoint.  It returns results along with a custom header for the endpoint we invoked."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pygmentize ../api/app.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `pre_traffic_hook.py` lambda is invoked prior to deployment and confirms the endpoint has data capture enabled."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pygmentize ../api/pre_traffic_hook.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `post_traffic_hook.py` lambda is invoked to perform any final checks, in this case to verify that we have received log data from data capature."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pygmentize ../api/post_traffic_hook.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the code below to fetch the name of the endpoint, then run a loop to wait for the endpoint to be fully deployed. You need the status to be 'InService'."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prd_endpoint_name='mlops-{}-prd-{}'.format(model_name, execution_id)\r\n",
    "print('prod endpoint: {}'.format(prd_endpoint_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sm = boto3.client('sagemaker')\r\n",
    "\r\n",
    "while True:\r\n",
    "    try:\r\n",
    "        response = sm.describe_endpoint(EndpointName=prd_endpoint_name)\r\n",
    "        print(\"Endpoint status: {}\".format(response['EndpointStatus']))\r\n",
    "        # Wait until the endpoint is in service with data capture enabled\r\n",
    "        if response['EndpointStatus'] == 'InService' \\\r\n",
    "            and 'DataCaptureConfig' in response \\\r\n",
    "            and response['DataCaptureConfig']['EnableCapture']:\r\n",
    "            break\r\n",
    "    except ClientError as e:\r\n",
    "        print(e.response[\"Error\"][\"Message\"])\r\n",
    "    time.sleep(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When the endpoint status is 'InService', you can continue. Earlier in this notebook, you created some code to send data to the dev endpoint. Reuse this code now to send a sample of the test data to the production endpoint. Since data capture is enabled on this endpoint, you want to send single records at a time, so the model monitor can map these records to the baseline. \n",
    "\n",
    "You will [inspect the model monitor](#Inspect-Model-Monitor) later in this notebook. For now, just check if you can send data to the endpoint and receive predictions in return."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prd_predictor = get_predictor(prd_endpoint_name)\r\n",
    "sample_values = test_df[test_df.columns[1:]].sample(100).values\r\n",
    "predictions = predict(prd_predictor, sample_values, rows=1)\r\n",
    "predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test REST API\n",
    "\n",
    "Although you already tested the SageMaker endpoint in the previous section, it is also a good idea to test the application created with API Gateway. \n",
    "\n",
    "![Traffic shift between endpoints](../docs/lambda-deploy-create.png)\n",
    "\n",
    "Follow the link below to open the Lambda Deployment where you can see the in-progress and completed deployments. You can also click to expand the **SAM template** to see the packaged CloudFormation template used in the deployment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/lambda/home?region={0}#/applications/{1}-deploy-prd?tab=deploy\">Lambda Deployment</a>'.format(region, model_name))"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the code below to confirm that the endpoint is in service.  It will complete once the REST API is available."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_stack_status(stack_name):\r\n",
    "    response = cfn.describe_stacks(StackName=stack_name)\r\n",
    "    if response['Stacks']:\r\n",
    "        stack = response['Stacks'][0]\r\n",
    "        outputs = None\r\n",
    "        if 'Outputs' in stack:\r\n",
    "            outputs = dict([(o['OutputKey'], o['OutputValue']) for o in stack['Outputs']])\r\n",
    "        return stack['StackStatus'], outputs \r\n",
    "\r\n",
    "outputs = None\r\n",
    "while True:\r\n",
    "    try:\r\n",
    "        status, outputs = get_stack_status(stack_name)\r\n",
    "        response = sm.describe_endpoint(EndpointName=prd_endpoint_name)\r\n",
    "        print(\"Endpoint status: {}\".format(response['EndpointStatus']))\r\n",
    "        if outputs:\r\n",
    "            break\r\n",
    "        elif status.endswith('FAILED'):\r\n",
    "            raise(Exception('Stack status: {}'.format(status)))\r\n",
    "    except ClientError as e:\r\n",
    "        print(e.response[\"Error\"][\"Message\"])\r\n",
    "    time.sleep(10)\r\n",
    "\r\n",
    "if outputs:\r\n",
    "    print('deployment application: {}'.format(outputs['DeploymentApplication']))\r\n",
    "    print('rest api: {}'.format(outputs['RestApi']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you are performing an update on your production deployment as a result of running [Trigger Retraining](#Trigger-Retraining) you will then be able to expand the Lambda Deployment tab to reveal the resources. Click on the **ApiFunctionAliaslive** link to see the Lambda Deployment in progress. \n",
    "\n",
    "![Traffic shift between endpoints](../docs/lambda-deploy-update.png)\n",
    "\n",
    "This page will be updated to list the deployment events.  It also has a link to the Deployment Application which you can access in the output of the next cell."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/codesuite/codedeploy/applications/{1}?region={0}\">CodeDeploy application</a>'.format(region, outputs['DeploymentApplication']))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "CodeDeploy will perform a canary deployment and send 10% of the traffic to the new endpoint over a 5-minute period.\n",
    "\n",
    "![Traffic shift between endpoints](../docs/code-deploy.gif)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can invoke the REST API and inspect the headers being returned to see which endpoint we are hitting.  You will occasionally see the cell below show a different endpoint that settles to the new version once the stack is complete.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "\r\n",
    "from urllib import request\r\n",
    "\r\n",
    "headers = {\"Content-type\": \"text/csv\"}\r\n",
    "payload = test_df[test_df.columns[1:]].head(1).to_csv(header=False, index=False).encode('utf-8')\r\n",
    "rest_api = outputs['RestApi']\r\n",
    "\r\n",
    "while True:\r\n",
    "    try:\r\n",
    "        resp = request.urlopen(request.Request(rest_api, data=payload, headers=headers))\r\n",
    "        print(\"Response code: %d: endpoint: %s\" % (resp.getcode(), resp.getheader('x-sagemaker-endpoint')))\r\n",
    "        status, outputs = get_stack_status(stack_name) \r\n",
    "        if status.endswith('COMPLETE'):\r\n",
    "            print('Deployment complete\\n')\r\n",
    "            break\r\n",
    "        elif status.endswith('FAILED'):\r\n",
    "            raise(Exception('Stack status: {}'.format(status)))\r\n",
    "    except ClientError as e:\r\n",
    "        print(e.response[\"Error\"][\"Message\"])\r\n",
    "    time.sleep(10)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Monitor\n",
    "\n",
    "### Inspect Model Monitor\n",
    "\n",
    "When you prepared the datasets for model training at the start of this notebook, you saved a baseline dataset (a copy of the train dataset). Then, when you approved the model for deployment into production, the pipeline set up an SageMaker Endpoint with data capture enabled and a model monitoring schedule. In this section, you will take a closer look at the model monitor results.\n",
    "\n",
    "To start off, fetch the latest production deployment execution ID."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "deploy_prd = get_pipeline_stage(pipeline_name, 'DeployPrd')\n",
    "if not 'latestExecution' in deploy_prd:\n",
    "    raise(Exception('Please wait.  Deploy prod not complete'))\n",
    "    \n",
    "execution_id = deploy_prd['latestExecution']['pipelineExecutionId']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Under the hood, SageMaker model monitor runs in SageMaker processing jobs. Use the execution ID to fetch the names of the processing job and the schedule."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "processing_job_name='mlops-{}-pbl-{}'.format(model_name, execution_id)\r\n",
    "schedule_name='mlops-{}-pms'.format(model_name)\r\n",
    "\r\n",
    "print('processing job name: {}'.format(processing_job_name))\r\n",
    "print('schedule name: {}'.format(schedule_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Explore Baseline\n",
    "\n",
    "Now fetch the baseline results from the processing job. This cell will throw an exception if the processing job is not complete - if that happens, just wait several minutes and try again. <a id=\"view-baseline-results\"></a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sagemaker\r\n",
    "from sagemaker.model_monitor import BaseliningJob, MonitoringExecution\r\n",
    "from sagemaker.s3 import S3Downloader\r\n",
    "\r\n",
    "sagemaker_session = sagemaker.Session()\r\n",
    "baseline_job = BaseliningJob.from_processing_name(sagemaker_session, processing_job_name)\r\n",
    "status = baseline_job.describe()['ProcessingJobStatus']\r\n",
    "if status != 'Completed':\r\n",
    "    raise(Exception('Please wait. Processing job not complete, status: {}'.format(status)))\r\n",
    "    \r\n",
    "baseline_results_uri  = baseline_job.outputs[0].destination"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SageMaker model monitor generates two types of files. Take a look at the statistics file first. It calculates various statistics for each feature of the dataset, including the mean, standard deviation, minimum value, maximum value, and more. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import json\r\n",
    "\r\n",
    "baseline_statistics = baseline_job.baseline_statistics().body_dict\r\n",
    "schema_df = pd.json_normalize(baseline_statistics[\"features\"])\r\n",
    "schema_df[[\"name\", \"numerical_statistics.mean\", \"numerical_statistics.std_dev\",\r\n",
    "           \"numerical_statistics.min\", \"numerical_statistics.max\"]].head()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now look at the suggested [constraints files](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-byoc-constraints.html)⇗. As the name implies, these are constraints which SageMaker model monitor recommends. If the live data which is sent to your production SageMaker Endpoint violates these constraints, this indicates data drift, and model monitor can raise an alert to trigger retraining. Of course, you can set different constraints based on the statistics which you viewed previously."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "baseline_constraints = baseline_job.suggested_constraints().body_dict\r\n",
    "constraints_df = pd.json_normalize(baseline_constraints[\"features\"])\r\n",
    "constraints_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### View data capture\n",
    "\n",
    "When the \"Deploy Production\" stage of the MLOps pipeline deploys a SageMaker endpoint, it also enables data capture. This means the incoming requests to the endpoint, as well as the results from the ML model, are stored in an S3 location. Model monitor can analyze this data and compare it to the baseline to ensure that no constraints are violated. \n",
    "\n",
    "Use the code below to check how many files have been created by the data capture, and view the latest file in detail. Note, data capture relies on data being sent to the production endpoint. If you don't see any files yet, wait several minutes and try again."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bucket = sagemaker_session.default_bucket()\r\n",
    "data_capture_logs_uri = 's3://{}/{}/datacapture/{}'.format(bucket, model_name, prd_endpoint_name)\r\n",
    "\r\n",
    "capture_files = S3Downloader.list(data_capture_logs_uri)\r\n",
    "print('Found {} files'.format(len(capture_files)))\r\n",
    "\r\n",
    "if capture_files:\r\n",
    "    # Get the first line of the most recent file    \r\n",
    "    event = json.loads(S3Downloader.read_file(capture_files[-1]).split('\\n')[0])\r\n",
    "    print('\\nLast file:\\n{}'.format(json.dumps(event, indent=2)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### View monitoring schedule\n",
    "\n",
    "There are some useful functions for plotting and rendering distribution statistics or constraint violations provided in a `utils` file in the [SageMaker Examples GitHub](https://github.com/aws/amazon-sagemaker-examples/tree/master/sagemaker_model_monitor/visualization)⇗. Grab a copy of this code to use in this notebook. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!wget -O utils.py --quiet https://raw.githubusercontent.com/awslabs/amazon-sagemaker-examples/master/sagemaker_model_monitor/visualization/utils.py\r\n",
    "import utils as mu"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The [minimum scheduled run time](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-scheduling.html)⇗ for model monitor is one hour, which means you will need to wait at least an hour to see any results. Use the code below to check the schedule status and list the next run. If you are completing this notebook as part of a workshop, your host will have activities which you can complete while you wait. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sm = boto3.client('sagemaker')\r\n",
    "\r\n",
    "response = sm.describe_monitoring_schedule(MonitoringScheduleName=schedule_name)\r\n",
    "print('Schedule Status: {}'.format(response['MonitoringScheduleStatus']))\r\n",
    "\r\n",
    "now = datetime.now(tzlocal())\r\n",
    "next_hour = (now+timedelta(hours=1)).replace(minute=0)\r\n",
    "scheduled_diff = (next_hour-now).seconds//60\r\n",
    "print('Next schedule in {} minutes'.format(scheduled_diff))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "While you wait, you can take a look at the CloudFormation template which is used as a base for the CloudFormation template built by CodeDeploy to deploy the production application. \n",
    "\n",
    "Alterntively, you can jump ahead to [Trigger Retraining](#Trigger-Retraining) which will kick off another run of the code pipeline whilst you wait."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!cat ../assets/deploy-model-prd.yml"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A couple of minutes after the model monitoring schedule has run, you can use the code below to fetch the latest schedule status.  A completed schedule run may have found violations. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "processing_job_arn = None\r\n",
    "\r\n",
    "while processing_job_arn == None:\r\n",
    "    try:\r\n",
    "        response = sm.list_monitoring_executions(MonitoringScheduleName=schedule_name)\r\n",
    "    except ClientError as e:\r\n",
    "        print(e.response[\"Error\"][\"Message\"])\r\n",
    "    for mon in response['MonitoringExecutionSummaries']:\r\n",
    "        status = mon['MonitoringExecutionStatus']\r\n",
    "        now = datetime.now(tzlocal())\r\n",
    "        created_diff = (now-mon['CreationTime']).seconds//60\r\n",
    "        print('Schedule status: {}, Created: {} minutes ago'.format(status, created_diff))\r\n",
    "        if status in ['Completed', 'CompletedWithViolations']:\r\n",
    "            processing_job_arn = mon['ProcessingJobArn']\r\n",
    "            break\r\n",
    "        if status == 'InProgress':\r\n",
    "            break\r\n",
    "    else:\r\n",
    "        raise(Exception('Please wait.  No Schedules executing'))\r\n",
    "    time.sleep(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### View monitoring results\n",
    "\n",
    "Once the model monitoring schedule has had a chance to run at least once, you can take a look at the results. First, load the monitoring execution results from the latest scheduled run."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if processing_job_arn:\r\n",
    "    execution = MonitoringExecution.from_processing_arn(sagemaker_session=sagemaker.Session(),\r\n",
    "                                                        processing_job_arn=processing_job_arn)\r\n",
    "    exec_inputs = {inp['InputName']: inp for inp in execution.describe()['ProcessingInputs']}\r\n",
    "    exec_results_uri = execution.output.destination\r\n",
    "\r\n",
    "    print('Monitoring Execution results: {}'.format(exec_results_uri))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Take a look at the files which have been saved in the S3 output location. If violations were found, you should see a constraint violations file in addition to the statistics and constraints file which you viewed before."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!aws s3 ls $exec_results_uri/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, fetch the monitoring statistics and violations. Then use the utils code to visualize the results in a table. It will highlight any baseline drift found by the model monitor. Drift can happen for categorical features (for inferred string styles) or for numerical features (e.g. total fare amount)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get the baseline and monitoring statistics & violations\r\n",
    "baseline_statistics = baseline_job.baseline_statistics().body_dict\r\n",
    "execution_statistics = execution.statistics().body_dict\r\n",
    "violations = execution.constraint_violations().body_dict['violations']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mu.show_violation_df(baseline_statistics=baseline_statistics, \r\n",
    "                     latest_statistics=execution_statistics, \r\n",
    "                     violations=violations)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trigger Retraining\n",
    "\n",
    "The CodePipeline instance is configured with [CloudWatch Events](https://docs.aws.amazon.com/codepipeline/latest/userguide/create-cloudtrail-S3-source.html)⇗ to start the pipeline for retraining when the drift detection triggers specific metric alarms.\n",
    "\n",
    "You can simulate drift by putting a metric value above the threshold of `0.2` directly into CloudWatch.  This will trigger the alarm, and start the code pipeline.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    Tip: This alarm is configured only for the latest production endpoint, so re-training will only occur if you are putting metrics against the latest endpoint.\n",
    "</div>\n",
    "\n",
    "![Metric graph in CloudWatch](../docs/cloudwatch-alarm.png)\n",
    "\n",
    "Run the code below to trigger the metric alarm. The cell output will be a link to CloudWatch, where you can see the alarm (similar to the screenshot above), and a link to CodePipeline which you will see run again. Note that it can take a couple of minutes for everything to trigger."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datetime import datetime\r\n",
    "import random\r\n",
    "\r\n",
    "cloudwatch = boto3.client('cloudwatch')\r\n",
    "\r\n",
    "# Define the metric name and threshold\r\n",
    "metric_name = 'feature_baseline_drift_total_amount'\r\n",
    "metric_threshold = 0.2\r\n",
    "\r\n",
    "# Put a new metric to trigger an alaram\r\n",
    "def put_drift_metric(value):\r\n",
    "    print('Putting metric: {}'.format(value))\r\n",
    "    response = cloudwatch.put_metric_data(\r\n",
    "        Namespace='aws/sagemaker/Endpoints/data-metrics',\r\n",
    "        MetricData=[\r\n",
    "            {\r\n",
    "                'MetricName': metric_name,\r\n",
    "                'Dimensions': [\r\n",
    "                    {\r\n",
    "                        'Name': 'MonitoringSchedule',\r\n",
    "                        'Value': schedule_name\r\n",
    "                    },\r\n",
    "                    {\r\n",
    "                        'Name': 'Endpoint',\r\n",
    "                        'Value': prd_endpoint_name\r\n",
    "                    },\r\n",
    "                ],\r\n",
    "                'Timestamp': datetime.now(),\r\n",
    "                'Value': value,\r\n",
    "                'Unit': 'None'\r\n",
    "            },\r\n",
    "        ]\r\n",
    "    )\r\n",
    "    \r\n",
    "def get_drift_stats():\r\n",
    "    response = cloudwatch.get_metric_statistics(\r\n",
    "        Namespace='aws/sagemaker/Endpoints/data-metrics',\r\n",
    "        MetricName=metric_name,\r\n",
    "        Dimensions=[\r\n",
    "            {\r\n",
    "                'Name': 'MonitoringSchedule',\r\n",
    "                'Value': schedule_name\r\n",
    "            },\r\n",
    "            {\r\n",
    "                'Name': 'Endpoint',\r\n",
    "                'Value': prd_endpoint_name\r\n",
    "            },\r\n",
    "        ],\r\n",
    "        StartTime=datetime.now() - timedelta(minutes=2),\r\n",
    "        EndTime=datetime.now(),\r\n",
    "        Period=1,\r\n",
    "        Statistics=['Average'],\r\n",
    "        Unit='None'\r\n",
    "    )\r\n",
    "    if 'Datapoints' in response and len(response['Datapoints']) > 0:        \r\n",
    "        return response['Datapoints'][0]['Average']\r\n",
    "    return 0    \r\n",
    "\r\n",
    "print('Simluate drift on endpoint: {}'.format(prd_endpoint_name))\r\n",
    "\r\n",
    "while True:\r\n",
    "    put_drift_metric(round(random.uniform(metric_threshold, 1.0), 4))\r\n",
    "    drift_stats = get_drift_stats()\r\n",
    "    print('Average drift amount: {}'.format(get_drift_stats()))\r\n",
    "    if drift_stats > metric_threshold:\r\n",
    "        break\r\n",
    "    time.sleep(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Click through to the Alarm and CodePipeline Execution history with the links below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Output a html link to the cloudwatch dashboard\r\n",
    "metric_alarm_name = 'mlops-{}-metric-gt-threshold'.format(model_name)\r\n",
    "HTML('''<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/cloudwatch/home?region={0}#alarmsV2:alarm/{1}\">CloudWatch Alarm</a> triggers\r\n",
    "     <a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/codesuite/codepipeline/pipelines/{2}/executions?region={0}\">Code Pipeline Execution</a>'''.format(region, metric_alarm_name, pipeline_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once the pipeline is running again you can jump back up to [Inspect Training Job](#Inspect-Training-Job)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Synthetic Monitoring\n",
    "\n",
    "[Amazon CloudWatch Synthetics](https://aws.amazon.com/blogs/aws/new-use-cloudwatch-synthetics-to-monitor-sites-api-endpoints-web-workflows-and-more/) allows you to monitor sites, REST APIs, and other services deployed on AWS. You can set up a canary to test that your REST API is returning an expected value at a regular interval. This is a great way to validate that the blue/green deployment is not causing any downtime for your end-users.\n",
    "\n",
    "Use the code below to set up a canary to continuously test the production deployment. This canary simply pings the REST API to test if it is live, using code from `notebook/canary.js`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from urllib.parse import urlparse\r\n",
    "from string import Template\r\n",
    "from io import BytesIO\r\n",
    "import zipfile\r\n",
    "\r\n",
    "# Format the canary_js with rest_api and payload\r\n",
    "rest_url = urlparse(rest_api)\r\n",
    "\r\n",
    "with open('canary.js') as f:\r\n",
    "    canary_js = Template(f.read()).substitute(hostname=rest_url.netloc, path=rest_url.path, \r\n",
    "                                              data=payload.decode('utf-8').strip())\r\n",
    "# Write the zip file\r\n",
    "zip_buffer = BytesIO()\r\n",
    "with zipfile.ZipFile(zip_buffer, 'w') as zf:\r\n",
    "    zip_path = 'nodejs/node_modules/apiCanaryBlueprint.js' # Set a valid path\r\n",
    "    zip_info = zipfile.ZipInfo(zip_path)\r\n",
    "    zip_info.external_attr = 0o0755 << 16 # Ensure the file is readable\r\n",
    "    zf.writestr(zip_info, canary_js)\r\n",
    "zip_buffer.seek(0)\r\n",
    "\r\n",
    "# Create the canary\r\n",
    "synth = boto3.client('synthetics')\r\n",
    "\r\n",
    "role = sagemaker.get_execution_role()\r\n",
    "s3_canary_uri = 's3://{}/{}'.format(artifact_bucket, model_name)\r\n",
    "canary_name = 'mlops-{}'.format(model_name)\r\n",
    "\r\n",
    "try:\r\n",
    "    response = synth.create_canary(\r\n",
    "        Name=canary_name,\r\n",
    "        Code={\r\n",
    "            'ZipFile': bytearray(zip_buffer.read()),\r\n",
    "            'Handler': 'apiCanaryBlueprint.handler'\r\n",
    "        },\r\n",
    "        ArtifactS3Location=s3_canary_uri,\r\n",
    "        ExecutionRoleArn=role,\r\n",
    "        Schedule={ \r\n",
    "            'Expression': 'rate(10 minutes)', \r\n",
    "            'DurationInSeconds': 0 },\r\n",
    "        RunConfig={\r\n",
    "            'TimeoutInSeconds': 60,\r\n",
    "            'MemoryInMB': 960\r\n",
    "        },\r\n",
    "        SuccessRetentionPeriodInDays=31,\r\n",
    "        FailureRetentionPeriodInDays=31,\r\n",
    "        RuntimeVersion='syn-nodejs-2.0',\r\n",
    "    )\r\n",
    "    print('Creating canary: {}'.format(canary_name))    \r\n",
    "except ClientError as e:\r\n",
    "    if e.response[\"Error\"][\"Code\"] == \"AccessDeniedException\":\r\n",
    "        print('Canary not supported.') # Not supported in event engine\r\n",
    "    else:\r\n",
    "        raise(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now create a CloudWatch alarm which will trigger if the success rate of the canary drops below 90%. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cloudwatch = boto3.client('cloudwatch')\r\n",
    "\r\n",
    "canary_alarm_name = '{}-synth-lt-threshold'.format(canary_name)\r\n",
    "\r\n",
    "response = cloudwatch.put_metric_alarm(\r\n",
    "    AlarmName=canary_alarm_name,\r\n",
    "    ComparisonOperator='LessThanThreshold',\r\n",
    "    EvaluationPeriods=1,\r\n",
    "    DatapointsToAlarm=1,\r\n",
    "    Period=600, # 10 minute interval\r\n",
    "    Statistic='Average',\r\n",
    "    Threshold=90.0,\r\n",
    "    ActionsEnabled=False,\r\n",
    "    AlarmDescription='SuccessPercent LessThanThreshold 90%',\r\n",
    "    Namespace='CloudWatchSynthetics',\r\n",
    "    MetricName='SuccessPercent',\r\n",
    "    Dimensions=[\r\n",
    "        {\r\n",
    "          'Name': 'CanaryName',\r\n",
    "          'Value': canary_name\r\n",
    "        },\r\n",
    "    ],\r\n",
    "    Unit='Seconds'\r\n",
    ")\r\n",
    "\r\n",
    "print('Creating alarm: {}'.format(canary_alarm_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the code below to check if the canary is running succesfully. The cell will output a link to your CloudWatch Canaries UI, where you can watch the results over time (see screenshot). It can take a couple of minutes for the canary to deploy.\n",
    "\n",
    "![Canary graph in CloudWatch](../docs/canary-green-1hr.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "while True:\r\n",
    "    try:\r\n",
    "        response = synth.get_canary(Name=canary_name)\r\n",
    "        status = response['Canary']['Status']['State']    \r\n",
    "        print('Canary status: {}'.format(status))\r\n",
    "        if status == 'ERROR':\r\n",
    "            raise(Exception(response['Canary']['Status']['StateReason']))    \r\n",
    "        elif status == 'READY':\r\n",
    "            synth.start_canary(Name=canary_name)\r\n",
    "        elif status == 'RUNNING':\r\n",
    "            break        \r\n",
    "    except ClientError as e:\r\n",
    "        if e.response[\"Error\"][\"Code\"] == \"ResourceNotFoundException\":\r\n",
    "            print('No canary found.')\r\n",
    "            break\r\n",
    "        elif e.response[\"Error\"][\"Code\"] == \"AccessDeniedException\":\r\n",
    "            print('Canary not supported.') # Not supported in event engine\r\n",
    "            break\r\n",
    "        print(e.response[\"Error\"][\"Message\"])\r\n",
    "    time.sleep(10)\r\n",
    "\r\n",
    "# Output a html link to the cloudwatch console\r\n",
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/cloudwatch/home?region={0}#synthetics:canary/detail/{1}\">CloudWatch Canary</a>'.format(region, canary_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a CloudWatch dashboard\n",
    "\n",
    "Finally, use the code below to create a CloudWatch dashboard to visualize the key performance metrics and alarms which you have created during this demo. The cell will output a link to the dashboard. This dashboard shows 9 charts in three rows, where the first row displays Lambda metrics, the second row displays SageMaker metrics, and the third row (shown in the screenshot below) displays the alarms set up for the pipeline.\n",
    "\n",
    "![Graphs in CloudWatch dashboard](../docs/cloudwatch-dashboard.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sts = boto3.client('sts')\r\n",
    "account_id = sts.get_caller_identity().get('Account')\r\n",
    "dashboard_name = 'mlops-{}'.format(model_name)\r\n",
    "\r\n",
    "with open('dashboard.json') as f:\r\n",
    "    dashboard_body = Template(f.read()).substitute(region=region, account_id=account_id, model_name=model_name)\r\n",
    "    response = cloudwatch.put_dashboard(\r\n",
    "        DashboardName=dashboard_name,\r\n",
    "        DashboardBody=dashboard_body\r\n",
    "    )\r\n",
    "\r\n",
    "# Output a html link to the cloudwatch dashboard\r\n",
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/cloudwatch/home?region={0}#dashboards:name={1}\">CloudWatch Dashboard</a>'.format(region, canary_name))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Congratulations! You have made it to the end of this notebook, and have automated a safe MLOps pipeline using a wide range of AWS services. \n",
    "\n",
    "You can use the other notebook in this repository [workflow.ipynb](workflow.ipynb) to implement your own ML model and deploy it as part of this pipeline. Or, if you are finished with the content, follow the instructions in the next section to clean up the resources you have deployed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cleanup\n",
    "\n",
    "Execute the following cell to delete the stacks created in the pipeline. For a model name of **nyctaxi** these would be:\n",
    "\n",
    "1. *nyctaxi*-deploy-prd\n",
    "2. *nyctaxi*-deploy-dev\n",
    "3. *nyctaxi*-workflow\n",
    "4. sagemaker-custom-resource"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cfn = boto3.client('cloudformation')\r\n",
    "\r\n",
    "# Delete the prod and then dev stack\r\n",
    "for stack_name in [f'{pipeline_name}-deploy-prd', \r\n",
    "                   f'{pipeline_name}-deploy-dev',\r\n",
    "                   f'{pipeline_name}-workflow',\r\n",
    "                   'sagemaker-custom-resource']:\r\n",
    "    print('Deleting stack: {}'.format(stack_name))\r\n",
    "    cfn.delete_stack(StackName=stack_name)\r\n",
    "    cfn.get_waiter('stack_delete_complete').wait(StackName=stack_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code will stop and delete the canary you created."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "while True:\r\n",
    "    try:\r\n",
    "        response = synth.get_canary(Name=canary_name)\r\n",
    "        status = response['Canary']['Status']['State']    \r\n",
    "        print('Canary status: {}'.format(status))\r\n",
    "        if status == 'ERROR':\r\n",
    "            raise(Exception(response['Canary']['Status']['StateReason']))    \r\n",
    "        elif status == 'STOPPED':\r\n",
    "            synth.delete_canary(Name=canary_name)\r\n",
    "        elif status == 'RUNNING':\r\n",
    "            synth.stop_canary(Name=canary_name)\r\n",
    "    except ClientError as e:\r\n",
    "        if e.response[\"Error\"][\"Code\"] == \"ResourceNotFoundException\":\r\n",
    "            print('Canary succesfully deleted.')\r\n",
    "            break\r\n",
    "        elif e.response[\"Error\"][\"Code\"] == \"AccessDeniedException\":\r\n",
    "            print('Canary not created.') # Not supported in event engine\r\n",
    "            break\r\n",
    "        print(e.response[\"Error\"][\"Message\"])\r\n",
    "    time.sleep(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code will delete the dashboard."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cloudwatch.delete_alarms(AlarmNames=[canary_alarm_name])\r\n",
    "print('Alarm deleted')\r\n",
    "\r\n",
    "cloudwatch.delete_dashboards(DashboardNames=[dashboard_name])\r\n",
    "print('Dashboard deleted')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, close this notebook and you can delete the CloudFormation you created to launch this MLOps sample."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}